{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b05e4cac-5e6d-410c-a4a7-dfd49a44aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (4.39.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\acer\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.6.3)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (2026.1.4)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (1.2.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
      "Requirement already satisfied: outcome in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.0.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->webdriver-manager) (3.4.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.23)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ertifi (C:\\Users\\Acer\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ertifi (C:\\Users\\Acer\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ertifi (C:\\Users\\Acer\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install selenium beautifulsoup4 pandas webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017207f1-4afe-453d-b299-de1bf7493a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Searching Uniqlo in New York, NY (North America) ---\n",
      "Found 0 stores in New York, NY.\n",
      "\n",
      "--- Searching Uniqlo in Toronto, ON (North America) ---\n",
      "Found 0 stores in Toronto, ON.\n",
      "\n",
      "--- Searching Uniqlo in San Francisco, CA (North America) ---\n",
      "Found 0 stores in San Francisco, CA.\n",
      "\n",
      "--- Searching Uniqlo in London, UK (Europe) ---\n",
      "Found 0 stores in London, UK.\n",
      "\n",
      "--- Searching Uniqlo in Paris, France (Europe) ---\n",
      "Found 0 stores in Paris, France.\n",
      "\n",
      "--- Searching Uniqlo in Berlin, Germany (Europe) ---\n",
      "Found 0 stores in Berlin, Germany.\n",
      "\n",
      "--- Searching Uniqlo in Tokyo, Japan (Asia) ---\n",
      "Found 0 stores in Tokyo, Japan.\n",
      "\n",
      "--- Searching Uniqlo in Singapore (Asia) ---\n",
      "Found 0 stores in Singapore.\n",
      "\n",
      "--- Searching Uniqlo in Kuala Lumpur (Asia) ---\n",
      "Found 0 stores in Kuala Lumpur.\n",
      "\n",
      "--- Searching Uniqlo in Melbourne, Australia (Oceania) ---\n",
      "Found 0 stores in Melbourne, Australia.\n",
      "\n",
      "--- Searching Uniqlo in Sydney, Australia (Oceania) ---\n",
      "Found 0 stores in Sydney, Australia.\n",
      "\n",
      "Scraping complete! Saved 0 reviews to uniqlo_global_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# We simulate \"Continents\" by searching major cities in those regions.\n",
    "# Yelp is most active in North America and Europe.\n",
    "LOCATIONS = {\n",
    "    \"North America\": [\"New York, NY\", \"Toronto, ON\", \"San Francisco, CA\"],\n",
    "    \"Europe\": [\"London, UK\", \"Paris, France\", \"Berlin, Germany\"],\n",
    "    \"Asia\": [\"Tokyo, Japan\", \"Singapore\", \"Kuala Lumpur\"], \n",
    "    \"Oceania\": [\"Melbourne, Australia\", \"Sydney, Australia\"]\n",
    "}\n",
    "\n",
    "OUTPUT_FILE = \"uniqlo_global_reviews.csv\"\n",
    "\n",
    "def get_driver():\n",
    "    \"\"\"Sets up the Selenium WebDriver with anti-detection options.\"\"\"\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless\")  # Run in background (Comment out to see the browser working)\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\") \n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def scrape_reviews_from_store(driver, store_url, continent, location_name):\n",
    "    \"\"\"Scrapes reviews from a specific store page.\"\"\"\n",
    "    reviews_data = []\n",
    "    driver.get(store_url)\n",
    "    time.sleep(random.uniform(3, 6)) # Random sleep to mimic human behavior\n",
    "\n",
    "    # Loop through pagination (Limit to first 3 pages per store to avoid bans)\n",
    "    page_count = 0\n",
    "    max_pages = 3 \n",
    "    \n",
    "    while page_count < max_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Yelp reviews are usually in list items inside a specific ul\n",
    "        # Note: Classes change dynamically, so we target generic structures where possible\n",
    "        reviews = soup.select('li > div.css-1qn0b6x') # This class selector often changes!\n",
    "        \n",
    "        # Fallback: simple finding if the specific class above fails\n",
    "        if not reviews:\n",
    "            reviews = soup.find_all('div', class_=lambda x: x and 'review' in x)\n",
    "\n",
    "        print(f\"   -> Found {len(reviews)} reviews on page {page_count + 1}...\")\n",
    "\n",
    "        for review in reviews:\n",
    "            try:\n",
    "                # 1. Author Name\n",
    "                author_tag = review.find('a', href=lambda x: x and '/user_details' in x)\n",
    "                author = author_tag.text.strip() if author_tag else \"Anonymous\"\n",
    "\n",
    "                # 2. Rating (Look for aria-label=\"5 star rating\")\n",
    "                rating_tag = review.find('div', role='img')\n",
    "                rating = rating_tag['aria-label'].split(' ')[0] if rating_tag and 'aria-label' in rating_tag.attrs else \"N/A\"\n",
    "\n",
    "                # 3. Date\n",
    "                # Yelp dates are often just text spans. We look for text that looks like a date.\n",
    "                date_tag = review.find('span', class_='css-chan6m') # Common date class, might need adjustment\n",
    "                published_at = date_tag.text.strip() if date_tag else \"N/A\"\n",
    "\n",
    "                # 4. Feedback (The actual text)\n",
    "                text_tag = review.find('p', class_=lambda x: x and 'comment' in x)\n",
    "                if not text_tag:\n",
    "                     text_tag = review.find('span', lang='en')\n",
    "                feedback = text_tag.text.strip() if text_tag else \"\"\n",
    "\n",
    "                # 5. Like Count (Helpful/Funny/Cool buttons)\n",
    "                # This is tricky as it varies. We look for the button text.\n",
    "                likes = 0\n",
    "                buttons = review.find_all('button')\n",
    "                for btn in buttons:\n",
    "                    if 'Useful' in btn.text or 'Helpful' in btn.text:\n",
    "                        # Extract number if present (e.g. \"Useful 2\")\n",
    "                        parts = btn.text.split()\n",
    "                        if len(parts) > 1 and parts[-1].isdigit():\n",
    "                            likes += int(parts[-1])\n",
    "\n",
    "                if feedback: # Only save if there is text\n",
    "                    reviews_data.append({\n",
    "                        \"Continent\": continent,\n",
    "                        \"Country/Location\": location_name,\n",
    "                        \"Author\": author,\n",
    "                        \"Rating\": rating,\n",
    "                        \"Published At\": published_at,\n",
    "                        \"Like Count\": likes,\n",
    "                        \"Feedback\": feedback\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                continue # Skip broken reviews\n",
    "\n",
    "        # Pagination Logic: Click \"Next\"\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, '//a[contains(@href, \"start=\") and contains(., \"Next\")]')\n",
    "            next_button.click()\n",
    "            time.sleep(random.uniform(3, 5))\n",
    "            page_count += 1\n",
    "        except:\n",
    "            break # No next button found\n",
    "\n",
    "    return reviews_data\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    all_reviews = []\n",
    "\n",
    "    try:\n",
    "        for continent, cities in LOCATIONS.items():\n",
    "            for city in cities:\n",
    "                print(f\"\\n--- Searching Uniqlo in {city} ({continent}) ---\")\n",
    "                \n",
    "                # 1. Search for Uniqlo in this city\n",
    "                search_url = f\"https://www.yelp.com/search?find_desc=Uniqlo&find_loc={city}\"\n",
    "                driver.get(search_url)\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "\n",
    "                # 2. Get links to the store pages from search results\n",
    "                # We target links that look like business pages (ignoring ads)\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                store_links = set()\n",
    "                \n",
    "                # Look for links containing /biz/uniqlo\n",
    "                for a in soup.find_all('a', href=True):\n",
    "                    if '/biz/uniqlo' in a['href'] and 'ad_business' not in a['href']:\n",
    "                        full_url = \"https://www.yelp.com\" + a['href'].split('?')[0]\n",
    "                        store_links.add(full_url)\n",
    "                \n",
    "                print(f\"Found {len(store_links)} stores in {city}.\")\n",
    "\n",
    "                # 3. Scrape each store found\n",
    "                for store_url in list(store_links)[:2]: # Limit to 2 stores per city for testing\n",
    "                    print(f\"Scraping store: {store_url}\")\n",
    "                    reviews = scrape_reviews_from_store(driver, store_url, continent, city)\n",
    "                    all_reviews.extend(reviews)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        \n",
    "        # Save to CSV\n",
    "        df = pd.DataFrame(all_reviews)\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"\\nScraping complete! Saved {len(df)} reviews to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a76ce2f1-6f97-4dbe-86f2-5685a3c9731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting undetected-chromedriver\n",
      "  Downloading undetected-chromedriver-3.5.5.tar.gz (65 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from undetected-chromedriver) (4.39.0)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from undetected-chromedriver) (2.32.5)\n",
      "Collecting websockets (from undetected-chromedriver)\n",
      "  Downloading websockets-16.0-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium>=4.9.0->undetected-chromedriver) (2.6.3)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2026.1.4)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->undetected-chromedriver) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->undetected-chromedriver) (3.11)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (2.0.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium>=4.9.0->undetected-chromedriver) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (2.23)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Downloading websockets-16.0-cp312-cp312-win_amd64.whl (178 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (setup.py): started\n",
      "  Building wheel for undetected-chromedriver (setup.py): finished with status 'done'\n",
      "  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.5-py3-none-any.whl size=47130 sha256=c9a61882cd085384dc8440e2080ab8547fdf2c32c8366c7a3047d9895e3154be\n",
      "  Stored in directory: c:\\users\\acer\\appdata\\local\\pip\\cache\\wheels\\c4\\f1\\aa\\9de6cf276210554d91e9c0526864563e850a428c5e76da4914\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: websockets, undetected-chromedriver\n",
      "Successfully installed undetected-chromedriver-3.5.5 websockets-16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ertifi (C:\\Users\\Acer\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ertifi (C:\\Users\\Acer\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ertifi (C:\\Users\\Acer\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install undetected-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54142d9-b25b-4e6a-937e-9fe58dad6440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing New York, NY ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-new-york-2\n",
      "   -> Collected 0 reviews.\n",
      "--- Processing Toronto, ON ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-toronto-5\n",
      "   -> Collected 0 reviews.\n",
      "--- Processing London, UK ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-london\n",
      "   -> Collected 0 reviews.\n",
      "--- Processing Paris, France ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-paris-5\n",
      "   -> Collected 0 reviews.\n",
      "--- Processing Tokyo, Japan ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-%E5%BE%A1%E5%BE%92%E7%94%BA%E5%BA%97-%E5%8F%B0%E6%9D%B1%E5%8C%BA\n",
      "   -> Collected 0 reviews.\n",
      "--- Processing Singapore ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-singapore-2\n",
      "   -> Collected 0 reviews.\n",
      "--- Processing Melbourne, Australia ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-melbourne\n",
      "   -> Collected 0 reviews.\n",
      "Done! Saved to uniqlo_global_reviews_stealth.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "LOCATIONS = {\n",
    "    \"North America\": [\"New York, NY\", \"Toronto, ON\"],\n",
    "    \"Europe\": [\"London, UK\", \"Paris, France\"],\n",
    "    \"Asia\": [\"Tokyo, Japan\", \"Singapore\"], \n",
    "    \"Oceania\": [\"Melbourne, Australia\"]\n",
    "}\n",
    "\n",
    "OUTPUT_FILE = \"uniqlo_global_reviews_stealth.csv\"\n",
    "\n",
    "def get_stealth_driver():\n",
    "    \"\"\"Sets up a driver that hides the fact it is a bot.\"\"\"\n",
    "    options = uc.ChromeOptions()\n",
    "    # options.add_argument('--headless') # Do NOT use headless for Yelp, it triggers bans\n",
    "    options.add_argument('--no-first-run')\n",
    "    \n",
    "    # Initialize the undetectable driver\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def human_scroll(driver):\n",
    "    \"\"\"Scrolls down the page slowly like a human reading.\"\"\"\n",
    "    total_height = int(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    for i in range(1, total_height, random.randint(300, 700)):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {i});\")\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "def scrape_reviews(driver, store_url, continent, location_name):\n",
    "    reviews_data = []\n",
    "    print(f\"   -> Accessing {store_url}\")\n",
    "    driver.get(store_url)\n",
    "    \n",
    "    # RANDOM WAIT: Critical to avoid blocking\n",
    "    time.sleep(random.uniform(5, 10))\n",
    "    human_scroll(driver)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Yelp layout changes frequently. We look for 'li' elements containing reviews.\n",
    "    # If the specific class fails, we fallback to a broader search.\n",
    "    reviews = soup.select('li > div.css-1qn0b6x')\n",
    "    if not reviews:\n",
    "        reviews = soup.find_all('div', class_=lambda x: x and 'review' in x)\n",
    "\n",
    "    for review in reviews:\n",
    "        try:\n",
    "            # Extract Author\n",
    "            author_tag = review.find('a', href=lambda x: x and '/user_details' in x)\n",
    "            author = author_tag.text.strip() if author_tag else \"Anonymous\"\n",
    "\n",
    "            # Extract Rating (looking for 'aria-label' in image div)\n",
    "            rating = \"N/A\"\n",
    "            rating_tag = review.find('div', role='img')\n",
    "            if rating_tag and 'aria-label' in rating_tag.attrs:\n",
    "                rating_str = rating_tag['aria-label']\n",
    "                if 'star rating' in rating_str:\n",
    "                    rating = rating_str.split(' ')[0]\n",
    "\n",
    "            # Extract Date\n",
    "            date_tag = review.find('span', class_='css-chan6m')\n",
    "            published_at = date_tag.text.strip() if date_tag else \"N/A\"\n",
    "\n",
    "            # Extract Feedback Text\n",
    "            text_tag = review.find('p', class_=lambda x: x and 'comment' in x)\n",
    "            if not text_tag: \n",
    "                text_tag = review.find('span', lang='en')\n",
    "            feedback = text_tag.text.strip() if text_tag else \"\"\n",
    "\n",
    "            # Extract Likes\n",
    "            likes = 0\n",
    "            buttons = review.find_all('button')\n",
    "            for btn in buttons:\n",
    "                text = btn.text\n",
    "                # Look for numbers in buttons like \"Useful 2\"\n",
    "                if any(k in text for k in ['Useful', 'Helpful', 'Cool']):\n",
    "                    parts = text.split()\n",
    "                    if parts and parts[-1].isdigit():\n",
    "                        likes += int(parts[-1])\n",
    "\n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": continent,\n",
    "                    \"Country\": location_name,\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return reviews_data\n",
    "\n",
    "def main():\n",
    "    driver = get_stealth_driver()\n",
    "    all_data = []\n",
    "    \n",
    "    try:\n",
    "        for continent, cities in LOCATIONS.items():\n",
    "            for city in cities:\n",
    "                print(f\"--- Processing {city} ---\")\n",
    "                \n",
    "                # 1. Search for Uniqlo in the city\n",
    "                driver.get(f\"https://www.yelp.com/search?find_desc=Uniqlo&find_loc={city}\")\n",
    "                time.sleep(random.uniform(5, 8))\n",
    "                \n",
    "                # 2. Find the first valid store link\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                store_link = None\n",
    "                for a in soup.find_all('a', href=True):\n",
    "                    if '/biz/uniqlo' in a['href'] and 'ad_business' not in a['href']:\n",
    "                        store_link = \"https://www.yelp.com\" + a['href'].split('?')[0]\n",
    "                        break # Just take the first valid store to be safe\n",
    "                \n",
    "                if store_link:\n",
    "                    # 3. Scrape the store\n",
    "                    data = scrape_reviews(driver, store_link, continent, city)\n",
    "                    all_data.extend(data)\n",
    "                    print(f\"   -> Collected {len(data)} reviews.\")\n",
    "                else:\n",
    "                    print(\"   -> No store found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"Done! Saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9cdd9f0-550f-4417-854b-5e0873603c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing New York, NY ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-new-york-2\n",
      "   -> Scanning 193 items for reviews...\n",
      "   -> Successfully extracted 0 reviews.\n",
      "--- Processing Toronto, ON ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-toronto-5\n",
      "   -> Scanning 147 items for reviews...\n",
      "   -> Successfully extracted 1 reviews.\n",
      "--- Processing London, UK ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-london\n",
      "   -> Scanning 64 items for reviews...\n",
      "   -> Successfully extracted 0 reviews.\n",
      "--- Processing Paris, France ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-paris-5\n",
      "   -> Scanning 61 items for reviews...\n",
      "   -> Successfully extracted 0 reviews.\n",
      "--- Processing Tokyo, Japan ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-%E5%BE%A1%E5%BE%92%E7%94%BA%E5%BA%97-%E5%8F%B0%E6%9D%B1%E5%8C%BA\n",
      "   -> Scanning 61 items for reviews...\n",
      "   -> Successfully extracted 0 reviews.\n",
      "--- Processing Singapore ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-singapore-2\n",
      "   -> Scanning 65 items for reviews...\n",
      "   -> Successfully extracted 0 reviews.\n",
      "--- Processing Melbourne, Australia ---\n",
      "   -> Accessing https://www.yelp.com/biz/uniqlo-melbourne\n",
      "   -> Scanning 79 items for reviews...\n",
      "   -> Successfully extracted 7 reviews.\n",
      "SUCCESS! Saved 8 reviews to uniqlo_global_reviews_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "LOCATIONS = {\n",
    "    \"North America\": [\"New York, NY\", \"Toronto, ON\"],\n",
    "    \"Europe\": [\"London, UK\", \"Paris, France\"],\n",
    "    \"Asia\": [\"Tokyo, Japan\", \"Singapore\"], \n",
    "    \"Oceania\": [\"Melbourne, Australia\"]\n",
    "}\n",
    "\n",
    "OUTPUT_FILE = \"uniqlo_global_reviews_fixed.csv\"\n",
    "\n",
    "def get_stealth_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    # options.add_argument('--headless') # Keep headless OFF for Yelp\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def human_scroll(driver):\n",
    "    \"\"\"Scrolls to trigger lazy loading.\"\"\"\n",
    "    total_height = int(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    for i in range(1, total_height, random.randint(400, 800)):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {i});\")\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "\n",
    "def scrape_reviews(driver, store_url, continent, location_name):\n",
    "    reviews_data = []\n",
    "    print(f\"   -> Accessing {store_url}\")\n",
    "    driver.get(store_url)\n",
    "    \n",
    "    # 1. Wait for reviews to load (Look for the text \"Reviews\")\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    except:\n",
    "        print(\"   -> Page timeout.\")\n",
    "        return []\n",
    "\n",
    "    time.sleep(random.uniform(3, 5))\n",
    "    human_scroll(driver)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # --- ROBUST FINDING STRATEGY ---\n",
    "    # Instead of looking for a specific class, we grab ALL list items (li)\n",
    "    # and check if they contain review-like data (Stars + Date).\n",
    "    \n",
    "    potential_reviews = soup.find_all('li')\n",
    "    print(f\"   -> Scanning {len(potential_reviews)} items for reviews...\")\n",
    "    \n",
    "    count = 0\n",
    "    for item in potential_reviews:\n",
    "        try:\n",
    "            # CHECK: Is this a review? \n",
    "            # It must have a div with 'star rating' in the aria-label\n",
    "            rating_div = item.find('div', role='img')\n",
    "            if not rating_div or not rating_div.get('aria-label'):\n",
    "                continue\n",
    "            \n",
    "            aria_text = rating_div.get('aria-label').lower()\n",
    "            if 'star rating' not in aria_text:\n",
    "                continue\n",
    "\n",
    "            # If we passed the check, extraction begins:\n",
    "            \n",
    "            # 1. Rating\n",
    "            rating = aria_text.split()[0] # \"5 star rating\" -> \"5\"\n",
    "\n",
    "            # 2. Author\n",
    "            author_tag = item.find('a', href=lambda x: x and '/user_details' in x)\n",
    "            author = author_tag.text.strip() if author_tag else \"Anonymous\"\n",
    "\n",
    "            # 3. Date (Look for text patterns common in dates)\n",
    "            # Yelp dates are often in a span with distinct styling. \n",
    "            # We look for the span immediately following the rating or user info.\n",
    "            published_at = \"N/A\"\n",
    "            # Strategy: Find all text spans, look for Date-like length\n",
    "            spans = item.find_all('span')\n",
    "            for s in spans:\n",
    "                txt = s.text.strip()\n",
    "                # Simple heuristic: Dates usually contain \"/\" or \",\" and are short\n",
    "                if len(txt) < 20 and (',' in txt or '/' in txt) and any(c.isdigit() for c in txt):\n",
    "                    # Exclude \"Useful\", \"Cool\" text\n",
    "                    if \"Useful\" not in txt and \"Cool\" not in txt:\n",
    "                        published_at = txt\n",
    "                        break\n",
    "\n",
    "            # 4. Feedback\n",
    "            # Review text usually has a 'lang' attribute (e.g., lang=\"en\")\n",
    "            text_tag = item.find('span', lang=True)\n",
    "            if not text_tag:\n",
    "                # Fallback: Look for the longest paragraph\n",
    "                paragraphs = item.find_all('p')\n",
    "                if paragraphs:\n",
    "                    text_tag = max(paragraphs, key=lambda p: len(p.text))\n",
    "            \n",
    "            feedback = text_tag.text.strip() if text_tag else \"\"\n",
    "\n",
    "            # 5. Like Count\n",
    "            likes = 0\n",
    "            buttons = item.find_all('button')\n",
    "            for btn in buttons:\n",
    "                btxt = btn.text\n",
    "                if any(x in btxt for x in ['Useful', 'Helpful', 'Cool']):\n",
    "                    parts = btxt.split()\n",
    "                    if parts and parts[-1].isdigit():\n",
    "                        likes += int(parts[-1])\n",
    "\n",
    "            # Only add if we found feedback text\n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": continent,\n",
    "                    \"Country\": location_name,\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "                count += 1\n",
    "                \n",
    "        except Exception:\n",
    "            continue # Skip bad items\n",
    "\n",
    "    return reviews_data\n",
    "\n",
    "def main():\n",
    "    driver = get_stealth_driver()\n",
    "    all_data = []\n",
    "    \n",
    "    try:\n",
    "        for continent, cities in LOCATIONS.items():\n",
    "            for city in cities:\n",
    "                print(f\"--- Processing {city} ---\")\n",
    "                \n",
    "                # Search\n",
    "                search_url = f\"https://www.yelp.com/search?find_desc=Uniqlo&find_loc={city}\"\n",
    "                driver.get(search_url)\n",
    "                time.sleep(5)\n",
    "                \n",
    "                # Find Link\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                store_link = None\n",
    "                for a in soup.find_all('a', href=True):\n",
    "                    if '/biz/uniqlo' in a['href'] and 'ad_business' not in a['href']:\n",
    "                        store_link = \"https://www.yelp.com\" + a['href'].split('?')[0]\n",
    "                        break \n",
    "                \n",
    "                if store_link:\n",
    "                    data = scrape_reviews(driver, store_link, continent, city)\n",
    "                    print(f\"   -> Successfully extracted {len(data)} reviews.\")\n",
    "                    all_data.extend(data)\n",
    "                else:\n",
    "                    print(\"   -> No store link found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        if all_data:\n",
    "            df = pd.DataFrame(all_data)\n",
    "            df.to_csv(OUTPUT_FILE, index=False)\n",
    "            print(f\"SUCCESS! Saved {len(df)} reviews to {OUTPUT_FILE}\")\n",
    "        else:\n",
    "            print(\"Failed to collect any data. Yelp might be blocking the page content.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77537b08-163b-4006-a849-26303e3e3b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\acer\\anaconda3\\lib\\site-packages (3.5.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from undetected-chromedriver) (4.39.0)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from undetected-chromedriver) (2.32.5)\n",
      "Requirement already satisfied: websockets in c:\\users\\acer\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (16.0)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium>=4.9.0->undetected-chromedriver) (2.6.3)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2026.1.4)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->undetected-chromedriver) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->undetected-chromedriver) (3.11)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (2.0.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium>=4.9.0->undetected-chromedriver) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (2.23)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ertifi (C:\\Users\\Acer\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ertifi (C:\\Users\\Acer\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ertifi (C:\\Users\\Acer\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install undetected-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e40225dd-0aec-4ad2-94e6-6a0caaca1d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROCESSING CONTINENT: Asia ===\n",
      "   -> Searching for Uniqlo stores in Tokyo, Japan...\n",
      "      Found 1 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-%E5%BE%A1%E5%BE%92%E7%94%BA%E5%BA%97-%E5%8F%B0%E6%9D%B1%E5%8C%BA\n",
      "      Collected 7 reviews. (Continent Total: 7)\n",
      "   -> Searching for Uniqlo stores in Osaka, Japan...\n",
      "      Found 1 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-%E5%A4%A7%E9%98%AA%E5%B8%82\n",
      "      Collected 2 reviews. (Continent Total: 9)\n",
      "   -> Searching for Uniqlo stores in Kyoto, Japan...\n",
      "      Found 0 stores.\n",
      "   -> Searching for Uniqlo stores in Yokohama, Japan...\n",
      "      Found 0 stores.\n",
      "   -> Searching for Uniqlo stores in Singapore...\n",
      "      Found 5 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-singapore-2\n",
      "      Collected 13 reviews. (Continent Total: 22)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-singapore-22\n",
      "      Collected 1 reviews. (Continent Total: 23)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-singapore-6\n",
      "      Collected 5 reviews. (Continent Total: 28)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-singapore-10\n",
      "      Collected 1 reviews. (Continent Total: 29)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-singapore-8\n",
      "      Collected 3 reviews. (Continent Total: 32)\n",
      "   -> Searching for Uniqlo stores in Manila, Philippines...\n",
      "      Found 8 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-makati-city\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-makati\n",
      "      Collected 1 reviews. (Continent Total: 33)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-taguig\n",
      "      Collected 2 reviews. (Continent Total: 35)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-manila\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-pasig\n",
      "      Collected 1 reviews. (Continent Total: 36)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-makati-3\n",
      "      Collected 3 reviews. (Continent Total: 39)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-%E3%83%A6%E3%83%8B%E3%82%AF%E3%83%AD-pasay\n",
      "      Collected 7 reviews. (Continent Total: 46)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-makati-2\n",
      "      Collected 1 reviews. (Continent Total: 47)\n",
      "   -> Searching for Uniqlo stores in Bangkok, Thailand...\n",
      "      Found 0 stores.\n",
      "   -> Searching for Uniqlo stores in Seoul, South Korea...\n",
      "      Found 0 stores.\n",
      "\n",
      "=== PROCESSING CONTINENT: North America ===\n",
      "   -> Searching for Uniqlo stores in New York, NY...\n",
      "      Found 6 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-new-york-14\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-new-york-2\n",
      "      Collected 30 reviews. (Continent Total: 30)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-brooklyn-3\n",
      "      Collected 20 reviews. (Continent Total: 50)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-new-york-20\n",
      "      Collected 1 reviews. (Continent Total: 51)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-soho-new-york-3\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-coffe-new-york\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Los Angeles, CA...\n",
      "      Found 5 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-glendale-3\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-the-bloc-los-angeles\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-los-angeles-10\n",
      "      Collected 10 reviews. (Continent Total: 61)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-culver-city\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-sherman-oaks\n",
      "      Collected 4 reviews. (Continent Total: 65)\n",
      "   -> Searching for Uniqlo stores in San Francisco, CA...\n",
      "      Found 2 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-daly-city\n",
      "      Collected 20 reviews. (Continent Total: 85)\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-san-francisco-5\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Chicago, IL...\n",
      "      Found 1 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-chicago-4\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Toronto, Canada...\n",
      "      Found 4 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-toronto-2\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-toronto-8\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-toronto-5\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-toronto-4\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Vancouver, Canada...\n",
      "      Found 2 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-burnaby\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-richmond-4\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Honolulu, HI...\n",
      "      Found 1 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-honolulu-8\n",
      "      No reviews found (or empty text).\n",
      "\n",
      "=== PROCESSING CONTINENT: Europe ===\n",
      "   -> Searching for Uniqlo stores in London, UK...\n",
      "      Found 5 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-london\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-london-8\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-london-19\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-london-4\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-london-3\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Paris, France...\n",
      "      Found 7 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-levallois-perret\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-paris-3\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-puteaux\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-paris\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-paris-5\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-paris-7\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-paris-6\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Berlin, Germany...\n",
      "      Found 5 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-berlin-4\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-berlin\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-berlin-6\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-berlin-3\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-berlin-5\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Milan, Italy...\n",
      "      Found 0 stores.\n",
      "   -> Searching for Uniqlo stores in Barcelona, Spain...\n",
      "      Found 1 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-barcelona\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Stockholm, Sweden...\n",
      "      Found 1 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-stockholm\n",
      "      No reviews found (or empty text).\n",
      "\n",
      "=== PROCESSING CONTINENT: Oceania ===\n",
      "   -> Searching for Uniqlo stores in Melbourne, Australia...\n",
      "      Found 3 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-chadstone\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-melbourne\n",
      "      No reviews found (or empty text).\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-malvern-east\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Sydney, Australia...\n",
      "      Found 1 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-sydney-2\n",
      "      No reviews found (or empty text).\n",
      "   -> Searching for Uniqlo stores in Brisbane, Australia...\n",
      "      Found 1 stores.\n",
      "   -> Scraping: https://www.yelp.com/biz/uniqlo-brisbane-3\n",
      "      No reviews found (or empty text).\n",
      "Script Finished. Data saved to uniqlo_global_reviews_comprehensive.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- 1. GLOBAL SEARCH LIST (The \"All Outlets\" Strategy) ---\n",
    "# We cover major hubs in every continent to approximate \"All Outlets\"\n",
    "LOCATIONS = {\n",
    "    \"Asia\": [\n",
    "        \"Tokyo, Japan\", \"Osaka, Japan\", \"Kyoto, Japan\", \"Yokohama, Japan\", \n",
    "        \"Singapore\", \"Manila, Philippines\", \"Bangkok, Thailand\", \"Seoul, South Korea\"\n",
    "    ],\n",
    "    \"North America\": [\n",
    "        \"New York, NY\", \"Los Angeles, CA\", \"San Francisco, CA\", \"Chicago, IL\", \n",
    "        \"Toronto, Canada\", \"Vancouver, Canada\", \"Honolulu, HI\"\n",
    "    ],\n",
    "    \"Europe\": [\n",
    "        \"London, UK\", \"Paris, France\", \"Berlin, Germany\", \"Milan, Italy\", \n",
    "        \"Barcelona, Spain\", \"Stockholm, Sweden\"\n",
    "    ],\n",
    "    \"Oceania\": [\n",
    "        \"Melbourne, Australia\", \"Sydney, Australia\", \"Brisbane, Australia\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "OUTPUT_FILE = \"uniqlo_global_reviews_comprehensive.csv\"\n",
    "TARGET_PER_CONTINENT = 500  # Stop after getting this many reviews per continent to balance data\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    # options.add_argument('--headless') # Keep Headless OFF to avoid detection\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def get_store_links(driver, city):\n",
    "    \"\"\"Searches Yelp for Uniqlo in a city and returns all store URLs found.\"\"\"\n",
    "    print(f\"   -> Searching for Uniqlo stores in {city}...\")\n",
    "    search_url = f\"https://www.yelp.com/search?find_desc=Uniqlo&find_loc={city}\"\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(5, 7))\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    store_links = set()\n",
    "    \n",
    "    # Extract links that look like business pages\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if '/biz/uniqlo' in a['href'] and 'ad_business' not in a['href']:\n",
    "            # Clean URL\n",
    "            full_url = \"https://www.yelp.com\" + a['href'].split('?')[0]\n",
    "            store_links.add(full_url)\n",
    "    \n",
    "    links = list(store_links)\n",
    "    print(f\"      Found {len(links)} stores.\")\n",
    "    return links\n",
    "\n",
    "def scrape_reviews(driver, store_url, continent, city):\n",
    "    \"\"\"Scrapes reviews from a single store URL.\"\"\"\n",
    "    reviews_data = []\n",
    "    \n",
    "    # Pagination: We try to scrape the first 3 pages (30 reviews) of every store found\n",
    "    # This prevents getting banned by scraping 100 pages of a single store\n",
    "    for start_num in [0, 10, 20]: \n",
    "        \n",
    "        paginated_url = f\"{store_url}?start={start_num}&sort_by=date_desc\"\n",
    "        driver.get(paginated_url)\n",
    "        time.sleep(random.uniform(4, 7))\n",
    "        \n",
    "        # Check for empty page or captcha\n",
    "        if \"human\" in driver.title.lower():\n",
    "            print(\"      !!! Captcha Detected - Waiting 30s !!!\")\n",
    "            time.sleep(30)\n",
    "            \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        list_items = soup.find_all('li')\n",
    "        \n",
    "        found_on_page = 0\n",
    "        \n",
    "        for item in list_items:\n",
    "            try:\n",
    "                # Validation: Must have star rating\n",
    "                rating_div = item.find('div', role='img')\n",
    "                if not rating_div or not rating_div.get('aria-label') or 'star rating' not in rating_div.get('aria-label'):\n",
    "                    continue\n",
    "\n",
    "                # Data Extraction\n",
    "                rating = rating_div.get('aria-label').split()[0] # \"5\"\n",
    "                \n",
    "                # Author\n",
    "                author_tag = item.find('a', href=lambda x: x and '/user_details' in x)\n",
    "                author = author_tag.text.strip() if author_tag else \"Anonymous\"\n",
    "                \n",
    "                # Date\n",
    "                published_at = \"N/A\"\n",
    "                for s in item.find_all('span'):\n",
    "                    txt = s.text.strip()\n",
    "                    if len(txt) < 20 and any(c.isdigit() for c in txt) and (',' in txt or '/' in txt):\n",
    "                        if \"Useful\" not in txt:\n",
    "                            published_at = txt\n",
    "                            break\n",
    "\n",
    "                # Feedback Text\n",
    "                text_tag = item.find('span', lang='en')\n",
    "                if not text_tag:\n",
    "                    # Fallback for non-English reviews (Japanese/French)\n",
    "                    text_tag = item.find('span', lang=True) \n",
    "                \n",
    "                feedback = text_tag.text.strip() if text_tag else \"\"\n",
    "\n",
    "                # Likes\n",
    "                likes = 0\n",
    "                for btn in item.find_all('button'):\n",
    "                    if any(x in btn.text for x in ['Useful', 'Helpful']):\n",
    "                        parts = btn.text.split()\n",
    "                        if parts and parts[-1].isdigit():\n",
    "                            likes += int(parts[-1])\n",
    "\n",
    "                if feedback:\n",
    "                    reviews_data.append({\n",
    "                        \"Continent\": continent,\n",
    "                        \"Country/City\": city,\n",
    "                        \"Store URL\": store_url,\n",
    "                        \"Author\": author,\n",
    "                        \"Rating\": rating,\n",
    "                        \"Published At\": published_at,\n",
    "                        \"Like Count\": likes,\n",
    "                        \"Feedback\": feedback\n",
    "                    })\n",
    "                    found_on_page += 1\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if found_on_page == 0:\n",
    "            break # Stop paging if no reviews found\n",
    "\n",
    "    return reviews_data\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    \n",
    "    # Initialize CSV if not exists\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country/City\", \"Store URL\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    try:\n",
    "        for continent, cities in LOCATIONS.items():\n",
    "            print(f\"\\n=== PROCESSING CONTINENT: {continent} ===\")\n",
    "            continent_count = 0\n",
    "            \n",
    "            for city in cities:\n",
    "                if continent_count >= TARGET_PER_CONTINENT:\n",
    "                    print(f\"--- Reached target for {continent}. Moving to next continent. ---\")\n",
    "                    break\n",
    "\n",
    "                # 1. Find Stores in this City\n",
    "                store_urls = get_store_links(driver, city)\n",
    "                \n",
    "                # 2. Scrape Each Store\n",
    "                for url in store_urls:\n",
    "                    print(f\"   -> Scraping: {url}\")\n",
    "                    new_reviews = scrape_reviews(driver, url, continent, city)\n",
    "                    \n",
    "                    if new_reviews:\n",
    "                        # Append to CSV\n",
    "                        df = pd.DataFrame(new_reviews)\n",
    "                        df.to_csv(OUTPUT_FILE, mode='a', header=False, index=False)\n",
    "                        continent_count += len(new_reviews)\n",
    "                        print(f\"      Collected {len(new_reviews)} reviews. (Continent Total: {continent_count})\")\n",
    "                    else:\n",
    "                        print(\"      No reviews found (or empty text).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"Script Finished. Data saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3643fd7-5027-4a15-8516-1a676de4a277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: https://www.uniqlo.com/my/en/products/E468971-000/reviews ---\n",
      "   -> Starting 'LOAD MORE' loop...\n",
      "   -> Closed Cookie Banner.\n",
      "      Clicked 'LOAD MORE' (1)...\n",
      "      Clicked 'LOAD MORE' (2)...\n",
      "      Clicked 'LOAD MORE' (3)...\n",
      "      Clicked 'LOAD MORE' (4)...\n",
      "      Clicked 'LOAD MORE' (5)...\n",
      "      Clicked 'LOAD MORE' (6)...\n",
      "      Clicked 'LOAD MORE' (7)...\n",
      "      Clicked 'LOAD MORE' (8)...\n",
      "      Clicked 'LOAD MORE' (9)...\n",
      "      Clicked 'LOAD MORE' (10)...\n",
      "      Clicked 'LOAD MORE' (11)...\n",
      "      Clicked 'LOAD MORE' (12)...\n",
      "      Clicked 'LOAD MORE' (13)...\n",
      "      Clicked 'LOAD MORE' (14)...\n",
      "      Clicked 'LOAD MORE' (15)...\n",
      "      Clicked 'LOAD MORE' (16)...\n",
      "      Clicked 'LOAD MORE' (17)...\n",
      "      Clicked 'LOAD MORE' (18)...\n",
      "      Clicked 'LOAD MORE' (19)...\n",
      "      Clicked 'LOAD MORE' (20)...\n",
      "      Clicked 'LOAD MORE' (21)...\n",
      "      Clicked 'LOAD MORE' (22)...\n",
      "      Clicked 'LOAD MORE' (23)...\n",
      "      Clicked 'LOAD MORE' (24)...\n",
      "   -> Loop ended: Message: script timeout\n",
      "  (Session info: chrome=143.0.7499.193)\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x1191213\n",
      "\t0x1191254\n",
      "\t0xf7e52b\n",
      "\t0x100eab6\n",
      "\t0xfeb4ec\n",
      "\t0x100db13\n",
      "\t0xfeb2e6\n",
      "\t0xfbd321\n",
      "\t0xfbe1d4\n",
      "\t0x13e5254\n",
      "\t0x13e080b\n",
      "\t0x13fd0ea\n",
      "\t0x11ab118\n",
      "\t0x11b311d\n",
      "\t0x1199518\n",
      "\t0x11996d9\n",
      "\t0x1183a68\n",
      "\t0x7661fcc9\n",
      "\t0x774c82ae\n",
      "\t0x774c827e\n",
      "\n",
      "   -> Extracting from 136 review blocks...\n",
      "   -> SUCCESS: Saved 130 reviews.\n",
      "\n",
      "--- Processing: https://www.uniqlo.com/my/en/products/E450310-000/reviews ---\n",
      "   -> Starting 'LOAD MORE' loop...\n",
      "   -> 'LOAD MORE' button not found (All reviews loaded).\n",
      "   -> Extracting from 0 review blocks...\n",
      "   -> No reviews found (Check if URL has reviews).\n",
      "\n",
      "--- Processing: https://www.uniqlo.com/my/en/products/E464023-000/reviews ---\n",
      "   -> Starting 'LOAD MORE' loop...\n",
      "   -> 'LOAD MORE' button not found (All reviews loaded).\n",
      "   -> Extracting from 0 review blocks...\n",
      "   -> No reviews found (Check if URL has reviews).\n",
      "\n",
      "--- Processing: https://www.uniqlo.com/my/en/products/E460324-000/reviews ---\n",
      "   -> Starting 'LOAD MORE' loop...\n",
      "   -> 'LOAD MORE' button not found (All reviews loaded).\n",
      "   -> Extracting from 0 review blocks...\n",
      "   -> No reviews found (Check if URL has reviews).\n",
      "\n",
      "--- Processing: https://www.uniqlo.com/my/en/products/E424873-000/reviews ---\n",
      "   -> Starting 'LOAD MORE' loop...\n",
      "      Clicked 'LOAD MORE' (1)...\n",
      "      Clicked 'LOAD MORE' (2)...\n",
      "      Clicked 'LOAD MORE' (3)...\n",
      "      Clicked 'LOAD MORE' (4)...\n",
      "      Clicked 'LOAD MORE' (5)...\n",
      "      Clicked 'LOAD MORE' (6)...\n",
      "      Clicked 'LOAD MORE' (7)...\n",
      "      Clicked 'LOAD MORE' (8)...\n",
      "      Clicked 'LOAD MORE' (9)...\n",
      "      Clicked 'LOAD MORE' (10)...\n",
      "      Clicked 'LOAD MORE' (11)...\n",
      "      Clicked 'LOAD MORE' (12)...\n",
      "      Clicked 'LOAD MORE' (13)...\n",
      "      Clicked 'LOAD MORE' (14)...\n",
      "      Clicked 'LOAD MORE' (15)...\n",
      "      Clicked 'LOAD MORE' (16)...\n",
      "      Clicked 'LOAD MORE' (17)...\n",
      "      Clicked 'LOAD MORE' (18)...\n",
      "      Clicked 'LOAD MORE' (19)...\n",
      "      Clicked 'LOAD MORE' (20)...\n",
      "      Clicked 'LOAD MORE' (21)...\n",
      "      Clicked 'LOAD MORE' (22)...\n",
      "      Clicked 'LOAD MORE' (23)...\n",
      "      Clicked 'LOAD MORE' (24)...\n",
      "      Clicked 'LOAD MORE' (25)...\n",
      "      Clicked 'LOAD MORE' (26)...\n",
      "      Clicked 'LOAD MORE' (27)...\n",
      "   -> Loop ended: Message: script timeout\n",
      "  (Session info: chrome=143.0.7499.193)\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x1191213\n",
      "\t0x1191254\n",
      "\t0xf7e52b\n",
      "\t0x100eab6\n",
      "\t0xfeb4ec\n",
      "\t0x100db13\n",
      "\t0xfeb2e6\n",
      "\t0xfbd321\n",
      "\t0xfbe1d4\n",
      "\t0x13e5254\n",
      "\t0x13e080b\n",
      "\t0x13fd0ea\n",
      "\t0x11ab118\n",
      "\t0x11b311d\n",
      "\t0x1199518\n",
      "\t0x11996d9\n",
      "\t0x1183a68\n",
      "\t0x7661fcc9\n",
      "\t0x774c82ae\n",
      "\t0x774c827e\n",
      "\n",
      "   -> Extracting from 146 review blocks...\n",
      "   -> SUCCESS: Saved 140 reviews.\n",
      "\n",
      "DONE. Data saved to uniqlo_malaysia_reviews_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_FILE = \"uniqlo_malaysia_reviews_fixed.csv\"\n",
    "\n",
    "# Add your product URLs here\n",
    "PRODUCT_URLS = [\n",
    "    \"https://www.uniqlo.com/my/en/products/E468971-000/reviews\",\n",
    "    \"https://www.uniqlo.com/my/en/products/E450310-000/reviews\", \n",
    "    \"https://www.uniqlo.com/my/en/products/E464023-000/reviews\", \n",
    "    \"https://www.uniqlo.com/my/en/products/E460324-000/reviews\",\n",
    "    \"https://www.uniqlo.com/my/en/products/E424873-000/reviews\",\n",
    "]\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    # options.add_argument('--headless') # Keep headless OFF so you can see it working\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def close_popups(driver):\n",
    "    \"\"\"Closes standard Uniqlo popups.\"\"\"\n",
    "    try:\n",
    "        # OneTrust Cookie Banner\n",
    "        WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "        ).click()\n",
    "        print(\"   -> Closed Cookie Banner.\")\n",
    "    except:\n",
    "        pass \n",
    "\n",
    "def click_load_more(driver):\n",
    "    \"\"\"\n",
    "    Revised logic based on your screenshot:\n",
    "    Finds the 'LOAD MORE' button and clicks it until it disappears.\n",
    "    \"\"\"\n",
    "    print(\"   -> Starting 'LOAD MORE' loop...\")\n",
    "    \n",
    "    # 1. Close popups first so they don't block clicks\n",
    "    close_popups(driver)\n",
    "    \n",
    "    click_count = 0\n",
    "    max_clicks = 100 # Safety limit\n",
    "    \n",
    "    while click_count < max_clicks:\n",
    "        try:\n",
    "            # Scroll to the bottom to make sure the button is in the viewport\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight - 1000);\")\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            # --- TARGETING THE BUTTON FROM YOUR IMAGE ---\n",
    "            # We look for ANY element containing exactly \"LOAD MORE\"\n",
    "            # We use XPath to be flexible (it works for divs, buttons, or spans)\n",
    "            load_more_btn = None\n",
    "            \n",
    "            # Try finding the specific text \"LOAD MORE\"\n",
    "            xpath_locators = [\n",
    "                \"//*[text()='LOAD MORE']\",           # Exact text match\n",
    "                \"//*[contains(text(), 'LOAD MORE')]\", # Contains text\n",
    "                \"//div[contains(@class, 'load-more')]\", # Common class name\n",
    "                \"//button[contains(@class, 'load-more')]\"\n",
    "            ]\n",
    "            \n",
    "            for xpath in xpath_locators:\n",
    "                try:\n",
    "                    element = driver.find_element(By.XPATH, xpath)\n",
    "                    if element.is_displayed():\n",
    "                        load_more_btn = element\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if load_more_btn:\n",
    "                # Use JavaScript click to force it (bypasses overlapping elements)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more_btn)\n",
    "                print(f\"      Clicked 'LOAD MORE' ({click_count+1})...\")\n",
    "                \n",
    "                # Wait for new reviews to load\n",
    "                time.sleep(3) \n",
    "                click_count += 1\n",
    "            else:\n",
    "                print(\"   -> 'LOAD MORE' button not found (All reviews loaded).\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   -> Loop ended: {e}\")\n",
    "            break\n",
    "\n",
    "def scrape_page(driver_source, product_url):\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    reviews_data = []\n",
    "    \n",
    "    # Check if reviews exist by looking for \"out of 5 stars\"\n",
    "    rating_elements = soup.find_all(string=lambda text: text and \"out of 5 stars\" in text)\n",
    "    print(f\"   -> Extracting from {len(rating_elements)} review blocks...\")\n",
    "\n",
    "    for rating_text in rating_elements:\n",
    "        try:\n",
    "            # Navigate up to the review container\n",
    "            container = rating_text.find_parent('div').find_parent('div') \n",
    "            \n",
    "            # 1. RATING\n",
    "            rating = rating_text.strip().split()[0] \n",
    "\n",
    "            # 2. DATE, FEEDBACK, AUTHOR\n",
    "            texts = list(container.stripped_strings)\n",
    "            \n",
    "            published_at = \"N/A\"\n",
    "            body = \"\"\n",
    "            author_info = \"Anonymous\"\n",
    "            likes = 0\n",
    "            \n",
    "            # Smart Text sorting\n",
    "            for t in texts:\n",
    "                # Date detection (dd/mm/yyyy)\n",
    "                if len(t) == 10 and t[2] == '/' and t[5] == '/':\n",
    "                    published_at = t\n",
    "                # Author Metadata (Age, Gender, Height)\n",
    "                elif any(k in t for k in [\"Male\", \"Female\", \"Height\", \"Weight\", \"Shoe size\"]):\n",
    "                    author_info = t\n",
    "                # Like Count\n",
    "                elif \"Helpful\" in t and \"(\" in t:\n",
    "                     try: likes = int(t.split('(')[1].split(')')[0])\n",
    "                     except: pass\n",
    "            \n",
    "            # The body is usually the longest text block that ISN'T the metadata\n",
    "            possible_bodies = [t for t in texts if t != author_info and len(t) > 5]\n",
    "            if possible_bodies:\n",
    "                body = max(possible_bodies, key=len)\n",
    "            \n",
    "            # Cleanup\n",
    "            if \"out of 5 stars\" in body: body = \"\" # unwanted capture\n",
    "\n",
    "            if body:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": \"Asia\",\n",
    "                    \"Country\": \"Malaysia\",\n",
    "                    \"Store URL\": product_url,\n",
    "                    \"Author\": author_info,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": body\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return reviews_data\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country\", \"Store URL\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    try:\n",
    "        for url in PRODUCT_URLS:\n",
    "            print(f\"\\n--- Processing: {url} ---\")\n",
    "            driver.get(url)\n",
    "            \n",
    "            # WAIT FOR PAGE TO FULLY LOAD\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # 1. Expand all reviews\n",
    "            click_load_more(driver)\n",
    "            \n",
    "            # 2. Scrape\n",
    "            reviews = scrape_page(driver.page_source, url)\n",
    "            \n",
    "            if reviews:\n",
    "                df = pd.DataFrame(reviews)\n",
    "                # Remove duplicates\n",
    "                df.drop_duplicates(subset=['Feedback'], inplace=True)\n",
    "                \n",
    "                df.to_csv(OUTPUT_FILE, mode='a', header=False, index=False)\n",
    "                print(f\"   -> SUCCESS: Saved {len(reviews)} reviews.\")\n",
    "            else:\n",
    "                print(\"   -> No reviews found (Check if URL has reviews).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"\\nDONE. Data saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18d05be0-5e93-4dd1-8a96-ad18a5b61f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged! Total reviews: 402\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your global Yelp data\n",
    "df_global = pd.read_csv(\"uniqlo_global_reviews_comprehensive.csv\")\n",
    "\n",
    "# Load your new Malaysia data\n",
    "df_my = pd.read_csv(\"uniqlo_malaysia_reviews_fixed.csv\")\n",
    "\n",
    "# Ensure columns match (The Malaysia script has an extra \"User Metadata\" column)\n",
    "# We can rename columns if they are slightly different\n",
    "df_my = df_my.rename(columns={\"Store/Product\": \"Store URL\"}) \n",
    "\n",
    "# Combine them\n",
    "df_combined = pd.concat([df_global, df_my], ignore_index=True)\n",
    "\n",
    "# Save the master file\n",
    "df_combined.to_csv(\"uniqlo_FINAL_MASTER_DATASET.csv\", index=False)\n",
    "print(f\"Merged! Total reviews: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51b1a917-cc9a-4fbf-b676-e3c2dc338365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping Page 1 ---\n",
      "   -> Found 67 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 2 ---\n",
      "   -> Found 67 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 3 ---\n",
      "   -> Found 67 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 4 ---\n",
      "   -> Found 67 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 5 ---\n",
      "   -> Found 67 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 6 ---\n",
      "   -> Found 67 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 7 ---\n",
      "   -> Found 67 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 8 ---\n",
      "   -> Found 67 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 9 ---\n",
      "   -> Found 67 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 10 ---\n",
      "   -> Redirected to Page 1. End of pages reached.\n",
      "\n",
      "DONE. Saved to trustpilot_uniqlo_eu_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_URL = \"https://www.trustpilot.com/review/uniqlo.eu\"\n",
    "OUTPUT_FILE = \"trustpilot_uniqlo_eu_reviews.csv\"\n",
    "TARGET_CONTINENT = \"Europe\"  # uniqlo.eu is Europe-based\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    # Trustpilot blocks headless often, so we run visible\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def scrape_trustpilot_page(driver_source):\n",
    "    \"\"\"Extracts reviews from the current HTML page.\"\"\"\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    reviews_data = []\n",
    "    \n",
    "    # Trustpilot reviews are in <article> tags or cards with specific styles\n",
    "    # We look for the main review card container\n",
    "    review_cards = soup.find_all('article')\n",
    "    \n",
    "    # Fallback if specific article tag isn't used\n",
    "    if not review_cards:\n",
    "        review_cards = soup.find_all('div', class_=lambda x: x and 'styles_reviewCard' in x)\n",
    "\n",
    "    print(f\"   -> Found {len(review_cards)} reviews on this page.\")\n",
    "\n",
    "    for card in review_cards:\n",
    "        try:\n",
    "            # 1. Author Name\n",
    "            author_tag = card.find('span', attrs={'data-consumer-name-typography': 'true'})\n",
    "            author = author_tag.text.strip() if author_tag else \"Anonymous\"\n",
    "\n",
    "            # 2. Location (Country)\n",
    "            # Trustpilot often puts this in a small gray text like \"DE\" or \"GB\"\n",
    "            country = \"Europe (General)\"\n",
    "            country_tag = card.find('div', class_=lambda x: x and 'typography_body-m' in x and 'consumer-information' in x) \n",
    "            if country_tag:\n",
    "                # Sometimes it is an SVG flag or text code\n",
    "                country = country_tag.text.strip()\n",
    "            \n",
    "            # Alternative: Check for \"Reviewed in X\" text if available\n",
    "            \n",
    "            # 3. Rating\n",
    "            # Look for the star image or data attribute\n",
    "            rating = \"N/A\"\n",
    "            star_div = card.find('div', attrs={'data-service-review-rating': True})\n",
    "            if star_div:\n",
    "                rating = star_div['data-service-review-rating']\n",
    "            else:\n",
    "                # Fallback: look for img alt text \"5 out of 5 stars\"\n",
    "                img = card.find('img', alt=lambda x: x and \"stars\" in x)\n",
    "                if img:\n",
    "                    rating = img['alt'].split()[0]\n",
    "\n",
    "            # 4. Date\n",
    "            # Trustpilot has \"Date of experience\" and \"Date of review\"\n",
    "            published_at = \"N/A\"\n",
    "            date_tag = card.find('time')\n",
    "            if date_tag and date_tag.has_attr('datetime'):\n",
    "                published_at = date_tag['datetime'].split('T')[0] # Get YYYY-MM-DD\n",
    "            \n",
    "            # 5. Feedback Text\n",
    "            # Look for the paragraph tag with specific typography\n",
    "            text_tag = card.find('p', attrs={'data-service-review-text-typography': 'true'})\n",
    "            feedback = text_tag.text.strip() if text_tag else \"\"\n",
    "            \n",
    "            # If there's a title, prepend it\n",
    "            title_tag = card.find('h2', attrs={'data-service-review-title-typography': 'true'})\n",
    "            if title_tag:\n",
    "                feedback = f\"{title_tag.text.strip()}. {feedback}\"\n",
    "\n",
    "            # 6. Like Count (Useful)\n",
    "            likes = 0\n",
    "            # Look for button text \"Useful 2\"\n",
    "            buttons = card.find_all('button')\n",
    "            for btn in buttons:\n",
    "                if \"Useful\" in btn.text:\n",
    "                    parts = btn.text.split()\n",
    "                    for p in parts:\n",
    "                        if p.isdigit():\n",
    "                            likes = int(p)\n",
    "                            break\n",
    "\n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": TARGET_CONTINENT,\n",
    "                    \"Country\": country,\n",
    "                    \"Source\": \"Trustpilot\",\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    return reviews_data\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    \n",
    "    # Initialize CSV\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country\", \"Source\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    try:\n",
    "        # Start at Page 1\n",
    "        current_page = 1\n",
    "        \n",
    "        while True:\n",
    "            url = f\"{BASE_URL}?page={current_page}\"\n",
    "            print(f\"\\n--- Scraping Page {current_page} ---\")\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for load\n",
    "            time.sleep(random.uniform(4, 7))\n",
    "            \n",
    "            # Check if page exists (Trustpilot redirects to page 1 if you go too far)\n",
    "            if current_page > 1 and \"page=1\" in driver.current_url:\n",
    "                print(\"   -> Redirected to Page 1. End of pages reached.\")\n",
    "                break\n",
    "            \n",
    "            # Scrape content\n",
    "            reviews = scrape_trustpilot_page(driver.page_source)\n",
    "            \n",
    "            if not reviews:\n",
    "                print(\"   -> No reviews found on this page. Stopping.\")\n",
    "                break\n",
    "                \n",
    "            # Save to CSV\n",
    "            df = pd.DataFrame(reviews)\n",
    "            df.to_csv(OUTPUT_FILE, mode='a', header=False, index=False)\n",
    "            print(f\"   -> Saved {len(reviews)} reviews.\")\n",
    "            \n",
    "            # Pagination Logic: Look for \"Next page\" button\n",
    "            try:\n",
    "                next_button = driver.find_element(By.NAME, \"pagination-button-next\")\n",
    "                if not next_button.is_enabled():\n",
    "                    print(\"   -> 'Next' button disabled. Finished.\")\n",
    "                    break\n",
    "            except:\n",
    "                # If button is missing, we check if we just scraped the last page\n",
    "                # Trustpilot sometimes just hides the button.\n",
    "                # We can also rely on the URL check at the start of the loop.\n",
    "                pass\n",
    "            \n",
    "            current_page += 1\n",
    "            \n",
    "            # Safety limit (Trustpilot has thousands of pages)\n",
    "            if current_page > 50: \n",
    "                print(\"   -> Reached safety limit of 50 pages.\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"\\nDONE. Saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38d5dc3c-0fa0-4577-94c7-3ad4a07baea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Merge Process ---\n",
      "Loaded Master File: 402 reviews\n",
      "Loaded Trustpilot File: 180 reviews\n",
      "\n",
      "--- MERGE SUCCESSFUL ---\n",
      "Total Combined Reviews: 582\n",
      "Saved to: UNIQLO_GLOBAL_REVIEWS_ALL.csv\n",
      "\n",
      "Data Breakdown by Continent:\n",
      "Continent\n",
      "Asia             317\n",
      "Europe           180\n",
      "North America     85\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- FILE NAMES ---\n",
    "# 1. Your existing master file (Yelp + Malaysia)\n",
    "file_1 = \"uniqlo_FINAL_MASTER_DATASET.csv\" \n",
    "\n",
    "# 2. Your new Trustpilot file\n",
    "file_2 = \"trustpilot_uniqlo_eu_reviews.csv\"\n",
    "\n",
    "# 3. The final output file name\n",
    "output_file = \"UNIQLO_GLOBAL_REVIEWS_ALL.csv\"\n",
    "\n",
    "def merge_datasets():\n",
    "    print(\"--- Starting Merge Process ---\")\n",
    "\n",
    "    # 1. Load the files\n",
    "    try:\n",
    "        df_master = pd.read_csv(file_1)\n",
    "        print(f\"Loaded Master File: {len(df_master)} reviews\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {file_1}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_trustpilot = pd.read_csv(file_2)\n",
    "        print(f\"Loaded Trustpilot File: {len(df_trustpilot)} reviews\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {file_2}\")\n",
    "        return\n",
    "\n",
    "    # 2. Standardize Columns\n",
    "    \n",
    "    if \"Source\" not in df_master.columns:\n",
    "        df_master[\"Source\"] = \"Official Website / Yelp\"\n",
    "        \n",
    "    if \"Source\" not in df_trustpilot.columns:\n",
    "        df_trustpilot[\"Source\"] = \"Trustpilot\"\n",
    "\n",
    "    # Ensure 'Like Count' is consistent (rename 'Likes' to 'Like Count' if needed)\n",
    "    if \"Likes\" in df_master.columns:\n",
    "        df_master = df_master.rename(columns={\"Likes\": \"Like Count\"})\n",
    "    if \"Likes\" in df_trustpilot.columns:\n",
    "        df_trustpilot = df_trustpilot.rename(columns={\"Likes\": \"Like Count\"})\n",
    "\n",
    "    # 3. Combine DataFrames\n",
    "    df_final = pd.concat([df_master, df_trustpilot], ignore_index=True)\n",
    "\n",
    "    # 4. Cleanup (Optional)\n",
    "    df_final = df_final.fillna(\"N/A\")\n",
    "\n",
    "    # 5. Save\n",
    "    df_final.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"\\n--- MERGE SUCCESSFUL ---\")\n",
    "    print(f\"Total Combined Reviews: {len(df_final)}\")\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "    \n",
    "    # 6. Show Breakdown\n",
    "    print(\"\\nData Breakdown by Continent:\")\n",
    "    print(df_final['Continent'].value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76a2baee-6290-440f-aa17-8fa3f18ec68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping Page 1 ---\n",
      "   -> Found 68 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 2 ---\n",
      "   -> Found 68 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 3 ---\n",
      "   -> Found 68 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 4 ---\n",
      "   -> Found 68 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 5 ---\n",
      "   -> Found 68 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 6 ---\n",
      "   -> Found 68 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 7 ---\n",
      "   -> Found 68 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 8 ---\n",
      "   -> Found 68 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 9 ---\n",
      "   -> Found 68 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 10 ---\n",
      "   -> Redirected to Page 1. Done.\n",
      "\n",
      "DONE. Saved to trustpilot_uniqlo_uk_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_URL = \"https://uk.trustpilot.com/review/www.uniqlo.com\"\n",
    "OUTPUT_FILE = \"trustpilot_uniqlo_uk_reviews.csv\"\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    # Visible browser is safer for Trustpilot\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def scrape_trustpilot_page(driver_source):\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    reviews_data = []\n",
    "    \n",
    "    # Find review cards (Trustpilot structure)\n",
    "    review_cards = soup.find_all('article')\n",
    "    \n",
    "    # Fallback if article tag fails\n",
    "    if not review_cards:\n",
    "        review_cards = soup.find_all('div', class_=lambda x: x and 'styles_reviewCard' in x)\n",
    "\n",
    "    print(f\"   -> Found {len(review_cards)} reviews on this page.\")\n",
    "\n",
    "    for card in review_cards:\n",
    "        try:\n",
    "            # 1. Author\n",
    "            author_tag = card.find('span', attrs={'data-consumer-name-typography': 'true'})\n",
    "            author = author_tag.text.strip() if author_tag else \"Anonymous\"\n",
    "\n",
    "            # 2. Rating (Look for star image alt text)\n",
    "            rating = \"N/A\"\n",
    "            img = card.find('img', alt=lambda x: x and \"stars\" in x)\n",
    "            if img:\n",
    "                rating = img['alt'].split()[0] # \"5 out of 5\" -> \"5\"\n",
    "            else:\n",
    "                # Backup: Look for data attribute\n",
    "                div = card.find('div', attrs={'data-service-review-rating': True})\n",
    "                if div: rating = div['data-service-review-rating']\n",
    "\n",
    "            # 3. Date\n",
    "            published_at = \"N/A\"\n",
    "            date_tag = card.find('time')\n",
    "            if date_tag and date_tag.has_attr('datetime'):\n",
    "                published_at = date_tag['datetime'].split('T')[0]\n",
    "\n",
    "            # 4. Feedback\n",
    "            text_tag = card.find('p', attrs={'data-service-review-text-typography': 'true'})\n",
    "            feedback = text_tag.text.strip() if text_tag else \"\"\n",
    "            \n",
    "            # Add Title to feedback if exists\n",
    "            title_tag = card.find('h2', attrs={'data-service-review-title-typography': 'true'})\n",
    "            if title_tag:\n",
    "                feedback = f\"{title_tag.text.strip()}. {feedback}\"\n",
    "\n",
    "            # 5. Like Count\n",
    "            likes = 0\n",
    "            buttons = card.find_all('button')\n",
    "            for btn in buttons:\n",
    "                if \"Useful\" in btn.text:\n",
    "                    parts = btn.text.split()\n",
    "                    for p in parts:\n",
    "                        if p.isdigit():\n",
    "                            likes = int(p)\n",
    "                            break\n",
    "\n",
    "            # Only add if there is actual text content\n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": \"Europe\",\n",
    "                    \"Country\": \"United Kingdom\",\n",
    "                    \"Source\": \"Trustpilot UK\",\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    return reviews_data\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    \n",
    "    # Initialize CSV\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country\", \"Source\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    try:\n",
    "        current_page = 1\n",
    "        \n",
    "        while True:\n",
    "            url = f\"{BASE_URL}?page={current_page}\"\n",
    "            print(f\"\\n--- Scraping Page {current_page} ---\")\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Random wait to act like a human\n",
    "            time.sleep(random.uniform(3, 6))\n",
    "            \n",
    "            # Check for Redirect (End of pages)\n",
    "            if current_page > 1 and \"page=1\" in driver.current_url:\n",
    "                print(\"   -> Redirected to Page 1. Done.\")\n",
    "                break\n",
    "            \n",
    "            # Check for \"human verification\" title\n",
    "            if \"blocked\" in driver.title.lower() or \"human\" in driver.title.lower():\n",
    "                print(\"   !!! Captcha Detected. Pausing 30s. Please solve it manually !!!\")\n",
    "                time.sleep(30)\n",
    "\n",
    "            # Scrape\n",
    "            reviews = scrape_trustpilot_page(driver.page_source)\n",
    "            \n",
    "            if not reviews:\n",
    "                print(\"   -> No reviews found. Stopping.\")\n",
    "                break\n",
    "                \n",
    "            # Save\n",
    "            df = pd.DataFrame(reviews)\n",
    "            df.to_csv(OUTPUT_FILE, mode='a', header=False, index=False)\n",
    "            print(f\"   -> Saved {len(reviews)} reviews.\")\n",
    "            \n",
    "            # Check for Next Button\n",
    "            try:\n",
    "                next_btn = driver.find_element(By.NAME, \"pagination-button-next\")\n",
    "                if not next_btn.is_enabled():\n",
    "                    print(\"   -> Next button disabled. Finished.\")\n",
    "                    break\n",
    "            except:\n",
    "                # If we scraped data but cant find button, assume it's the last page or hidden\n",
    "                pass\n",
    "            \n",
    "            current_page += 1\n",
    "            \n",
    "            # Safety Cap (Trustpilot UK has lots of pages, adjust if needed)\n",
    "            if current_page > 100: \n",
    "                print(\"   -> Reached limit of 100 pages.\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"\\nDONE. Saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f92c1f4-be8c-4b05-8de5-faeebe510736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dataset! Total Reviews: 762\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your current big file (Global + Malaysia + EU)\n",
    "df_all = pd.read_csv(\"UNIQLO_GLOBAL_REVIEWS_ALL.csv\")\n",
    "\n",
    "# 2. Load the new UK file\n",
    "df_uk = pd.read_csv(\"trustpilot_uniqlo_uk_reviews.csv\")\n",
    "\n",
    "# 3. Combine\n",
    "df_final = pd.concat([df_all, df_uk], ignore_index=True)\n",
    "\n",
    "# 4. Save\n",
    "df_final.to_csv(\"UNIQLO_GLOBAL_REVIEWS_ALL_V2.csv\", index=False)\n",
    "print(f\"Updated Dataset! Total Reviews: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e58ede40-1196-421e-978d-07cea00d3d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping Page 1 ---\n",
      "   -> Found 24 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 2 ---\n",
      "   -> Found 24 reviews on this page.\n",
      "   -> Saved 20 reviews.\n",
      "\n",
      "--- Scraping Page 3 ---\n",
      "   -> Found 5 reviews on this page.\n",
      "   -> Saved 1 reviews.\n",
      "\n",
      "--- Scraping Page 4 ---\n",
      "   -> Found 0 reviews on this page.\n",
      "   -> No reviews found. Stopping.\n",
      "\n",
      "DONE. Saved to trustpilot_uniqlo_dk_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# We use the specific URL you provided\n",
    "BASE_URL = \"https://au.trustpilot.com/review/uniqlo.dk\"\n",
    "OUTPUT_FILE = \"trustpilot_uniqlo_dk_reviews.csv\"\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def scrape_trustpilot_page(driver_source):\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    reviews_data = []\n",
    "    \n",
    "    # Find review cards\n",
    "    review_cards = soup.find_all('article')\n",
    "    if not review_cards:\n",
    "        review_cards = soup.find_all('div', class_=lambda x: x and 'styles_reviewCard' in x)\n",
    "\n",
    "    print(f\"   -> Found {len(review_cards)} reviews on this page.\")\n",
    "\n",
    "    for card in review_cards:\n",
    "        try:\n",
    "            # 1. Author\n",
    "            author_tag = card.find('span', attrs={'data-consumer-name-typography': 'true'})\n",
    "            author = author_tag.text.strip() if author_tag else \"Anonymous\"\n",
    "\n",
    "            # 2. Rating\n",
    "            rating = \"N/A\"\n",
    "            img = card.find('img', alt=lambda x: x and \"stars\" in x)\n",
    "            if img:\n",
    "                rating = img['alt'].split()[0] # \"5 out of 5\" -> \"5\"\n",
    "            else:\n",
    "                div = card.find('div', attrs={'data-service-review-rating': True})\n",
    "                if div: rating = div['data-service-review-rating']\n",
    "\n",
    "            # 3. Date\n",
    "            published_at = \"N/A\"\n",
    "            date_tag = card.find('time')\n",
    "            if date_tag and date_tag.has_attr('datetime'):\n",
    "                published_at = date_tag['datetime'].split('T')[0]\n",
    "\n",
    "            # 4. Feedback\n",
    "            text_tag = card.find('p', attrs={'data-service-review-text-typography': 'true'})\n",
    "            feedback = text_tag.text.strip() if text_tag else \"\"\n",
    "            \n",
    "            # Prepend Title\n",
    "            title_tag = card.find('h2', attrs={'data-service-review-title-typography': 'true'})\n",
    "            if title_tag:\n",
    "                feedback = f\"{title_tag.text.strip()}. {feedback}\"\n",
    "\n",
    "            # 5. Like Count\n",
    "            likes = 0\n",
    "            buttons = card.find_all('button')\n",
    "            for btn in buttons:\n",
    "                if \"Useful\" in btn.text:\n",
    "                    parts = btn.text.split()\n",
    "                    for p in parts:\n",
    "                        if p.isdigit():\n",
    "                            likes = int(p)\n",
    "                            break\n",
    "\n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": \"Europe\",\n",
    "                    \"Country\": \"Denmark\",\n",
    "                    \"Source\": \"Trustpilot Denmark\",\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    return reviews_data\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country\", \"Source\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    try:\n",
    "        current_page = 1\n",
    "        \n",
    "        while True:\n",
    "            url = f\"{BASE_URL}?page={current_page}\"\n",
    "            print(f\"\\n--- Scraping Page {current_page} ---\")\n",
    "            driver.get(url)\n",
    "            \n",
    "            time.sleep(random.uniform(3, 6))\n",
    "            \n",
    "            # Check for Redirect (End of pages)\n",
    "            if current_page > 1 and \"page=1\" in driver.current_url:\n",
    "                print(\"   -> Redirected to Page 1. Done.\")\n",
    "                break\n",
    "            \n",
    "            # Scrape\n",
    "            reviews = scrape_trustpilot_page(driver.page_source)\n",
    "            \n",
    "            if not reviews:\n",
    "                print(\"   -> No reviews found. Stopping.\")\n",
    "                break\n",
    "                \n",
    "            # Save\n",
    "            df = pd.DataFrame(reviews)\n",
    "            df.to_csv(OUTPUT_FILE, mode='a', header=False, index=False)\n",
    "            print(f\"   -> Saved {len(reviews)} reviews.\")\n",
    "            \n",
    "            # Check for Next Button\n",
    "            try:\n",
    "                next_btn = driver.find_element(By.NAME, \"pagination-button-next\")\n",
    "                if not next_btn.is_enabled():\n",
    "                    print(\"   -> Next button disabled. Finished.\")\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            current_page += 1\n",
    "            \n",
    "            # Safety limit\n",
    "            if current_page > 50: \n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"\\nDONE. Saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "188cfa20-f848-409c-8346-cb065b4e656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing data: 762 reviews\n",
      "Loaded Denmark data: 41 reviews\n",
      "SUCCESS! New Total: 803\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your current V2 dataset (Global + EU + UK)\n",
    "try:\n",
    "    df_all = pd.read_csv(\"UNIQLO_GLOBAL_REVIEWS_ALL_V2.csv\")\n",
    "    print(f\"Loaded existing data: {len(df_all)} reviews\")\n",
    "except:\n",
    "    print(\"Could not find previous file. Starting fresh.\")\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "# 2. Load the new Denmark file\n",
    "df_dk = pd.read_csv(\"trustpilot_uniqlo_dk_reviews.csv\")\n",
    "print(f\"Loaded Denmark data: {len(df_dk)} reviews\")\n",
    "\n",
    "# 3. Combine\n",
    "df_final = pd.concat([df_all, df_dk], ignore_index=True)\n",
    "\n",
    "# 4. Save V3\n",
    "df_final.to_csv(\"UNIQLO_GLOBAL_REVIEWS_ALL_V3.csv\", index=False)\n",
    "print(f\"SUCCESS! New Total: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf842e88-b916-4f81-a4a4-e8413cd84baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Accessing https://www.uniqlo.com/my/en/products/E479078-000/reviews ---\n",
      "   -> Checking for Cookie Banner...\n",
      "      No banner found or already closed.\n",
      "   -> Starting to load reviews...\n",
      "      Clicked 'View more' (1)\n",
      "      Clicked 'View more' (2)\n",
      "      Clicked 'View more' (3)\n",
      "      Clicked 'View more' (4)\n",
      "      Clicked 'View more' (5)\n",
      "      Clicked 'View more' (6)\n",
      "      Clicked 'View more' (7)\n",
      "      Clicked 'View more' (8)\n",
      "      Clicked 'View more' (9)\n",
      "      Clicked 'View more' (10)\n",
      "   -> No more 'View more' buttons found. All loaded.\n",
      "   -> Found 60 review blocks.\n",
      "SUCCESS! Saved 54 reviews to uniqlo_malaysia_new_product_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_FILE = \"uniqlo_malaysia_new_product_reviews.csv\"\n",
    "PRODUCT_URL = \"https://www.uniqlo.com/my/en/products/E479078-000/reviews\"\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    options.page_load_strategy = 'normal'\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def close_cookie_banner(driver):\n",
    "    \"\"\"Closes the OneTrust cookie banner if present.\"\"\"\n",
    "    print(\"   -> Checking for Cookie Banner...\")\n",
    "    try:\n",
    "        btn = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        print(\"      Banner Closed.\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"      No banner found or already closed.\")\n",
    "\n",
    "def load_all_reviews(driver):\n",
    "    \"\"\"Clicks 'View more' until all reviews are loaded.\"\"\"\n",
    "    print(\"   -> Starting to load reviews...\")\n",
    "    click_count = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Scroll to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight - 800);\")\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            # Look for \"View more\" button (Based on your snippet)\n",
    "            # Try multiple selectors to be safe\n",
    "            load_btn = None\n",
    "            xpath_list = [\n",
    "                \"//button[contains(text(), 'View more')]\",\n",
    "                \"//button[contains(text(), 'View More')]\", \n",
    "                \"//div[contains(@class, 'load-more')]\",\n",
    "                \"//button[contains(@class, 'bv-content-btn-load-more')]\" \n",
    "            ]\n",
    "            \n",
    "            for xpath in xpath_list:\n",
    "                try:\n",
    "                    btn = driver.find_element(By.XPATH, xpath)\n",
    "                    if btn.is_displayed():\n",
    "                        load_btn = btn\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if load_btn:\n",
    "                driver.execute_script(\"arguments[0].click();\", load_btn)\n",
    "                print(f\"      Clicked 'View more' ({click_count + 1})\")\n",
    "                time.sleep(3) # Wait for content to load\n",
    "                click_count += 1\n",
    "            else:\n",
    "                print(\"   -> No more 'View more' buttons found. All loaded.\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   -> Loading loop stopped: {e}\")\n",
    "            break\n",
    "\n",
    "def scrape_page(driver_source):\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    reviews_data = []\n",
    "    \n",
    "    # Uniqlo MY reviews usually contain \"out of 5 stars\"\n",
    "    # We find these text nodes and work upwards to the container\n",
    "    rating_texts = soup.find_all(string=lambda text: text and \"out of 5 stars\" in text)\n",
    "    \n",
    "    print(f\"   -> Found {len(rating_texts)} review blocks.\")\n",
    "    \n",
    "    for r_text in rating_texts:\n",
    "        try:\n",
    "            # Navigate to container\n",
    "            container = r_text.find_parent('div').find_parent('div')\n",
    "            if not container: continue\n",
    "            \n",
    "            texts = list(container.stripped_strings)\n",
    "            \n",
    "            # --- Extract Fields ---\n",
    "            rating = r_text.strip().split()[0] # \"5\"\n",
    "            \n",
    "            published_at = \"N/A\"\n",
    "            author = \"Anonymous\"\n",
    "            feedback = \"\"\n",
    "            likes = 0\n",
    "            \n",
    "            for t in texts:\n",
    "                # Date (dd/mm/yyyy)\n",
    "                if len(t) == 10 and t[2] == '/' and t[5] == '/':\n",
    "                    published_at = t\n",
    "                # Author Metadata\n",
    "                if any(k in t for k in [\"Male\", \"Female\", \"Height\", \"Weight\", \"Shoe size\"]):\n",
    "                    author = t\n",
    "                # Like Count\n",
    "                if \"Helpful\" in t and \"(\" in t:\n",
    "                     try: likes = int(t.split('(')[1].split(')')[0])\n",
    "                     except: pass\n",
    "            \n",
    "            # Feedback is longest text that isn't metadata\n",
    "            possible_bodies = [\n",
    "                t for t in texts \n",
    "                if len(t) > 10 \n",
    "                and t != author \n",
    "                and \"out of 5 stars\" not in t \n",
    "                and \"Purchased size\" not in t\n",
    "            ]\n",
    "            if possible_bodies:\n",
    "                feedback = max(possible_bodies, key=len)\n",
    "            \n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": \"Asia\",\n",
    "                    \"Country\": \"Malaysia\",\n",
    "                    \"Source\": \"Uniqlo MY Official\",\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "                \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return reviews_data\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country\", \"Source\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "        \n",
    "    try:\n",
    "        print(f\"--- Accessing {PRODUCT_URL} ---\")\n",
    "        driver.get(PRODUCT_URL)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        close_cookie_banner(driver)\n",
    "        load_all_reviews(driver)\n",
    "        \n",
    "        reviews = scrape_page(driver.page_source)\n",
    "        \n",
    "        if reviews:\n",
    "            df = pd.DataFrame(reviews)\n",
    "            # Remove duplicates\n",
    "            df.drop_duplicates(subset=['Feedback'], inplace=True)\n",
    "            df.to_csv(OUTPUT_FILE, mode='a', header=False, index=False)\n",
    "            print(f\"SUCCESS! Saved {len(reviews)} reviews to {OUTPUT_FILE}\")\n",
    "        else:\n",
    "            print(\"No reviews found. Page structure might differ.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6417be9b-e2d0-4407-aa1b-8c7dfad7dcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged! New Total: 857\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your main V4 (or V5) dataset\n",
    "df_main = pd.read_csv(\"UNIQLO_GLOBAL_REVIEWS_ALL_V3.csv\") \n",
    "\n",
    "# Load the new Malaysia file\n",
    "df_new_my = pd.read_csv(\"uniqlo_malaysia_new_product_reviews.csv\")\n",
    "\n",
    "# Combine\n",
    "df_final = pd.concat([df_main, df_new_my], ignore_index=True)\n",
    "\n",
    "# Save\n",
    "df_final.to_csv(\"UNIQLO_GLOBAL_REVIEWS_ALL_FINAL.csv\", index=False)\n",
    "print(f\"Merged! New Total: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80fa3396-c09b-4470-b8bf-03a724b2682e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Accessing https://www.uniqlo.com/my/en/products/E476602-000/reviews ---\n",
      "   -> Checking for Cookie Banner...\n",
      "      Banner Closed.\n",
      "   -> Hunting for the 'Load more' button inside iframes...\n",
      "      Found button on MAIN PAGE.\n",
      "      Clicked 'Load more' (1)\n",
      "      Clicked 'Load more' (2)\n",
      "      Clicked 'Load more' (3)\n",
      "      Clicked 'Load more' (4)\n",
      "      Clicked 'Load more' (5)\n",
      "      Clicked 'Load more' (6)\n",
      "      Clicked 'Load more' (7)\n",
      "      Clicked 'Load more' (8)\n",
      "      Clicked 'Load more' (9)\n",
      "      Clicked 'Load more' (10)\n",
      "      Clicked 'Load more' (11)\n",
      "      Clicked 'Load more' (12)\n",
      "      Clicked 'Load more' (13)\n",
      "      Clicked 'Load more' (14)\n",
      "      Clicked 'Load more' (15)\n",
      "      Clicked 'Load more' (16)\n",
      "      Clicked 'Load more' (17)\n",
      "      Clicked 'Load more' (18)\n",
      "      Clicked 'Load more' (19)\n",
      "      Clicked 'Load more' (20)\n",
      "      Clicked 'Load more' (21)\n",
      "      Clicked 'Load more' (22)\n",
      "      Button gone. All reviews loaded.\n",
      "   -> Parsing final data...\n",
      "   -> Extracting... Found 126 review blocks.\n",
      "SUCCESS! Saved 120 reviews to uniqlo_my_sweatshirt_reviews.csv\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_FILE = \"uniqlo_my_sweatshirt_reviews.csv\"\n",
    "PRODUCT_URL = \"https://www.uniqlo.com/my/en/products/E476602-000/reviews\"\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    options.page_load_strategy = 'normal'\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def close_cookie_banner(driver):\n",
    "    \"\"\"Closes the pop-up banner that blocks buttons.\"\"\"\n",
    "    print(\"   -> Checking for Cookie Banner...\")\n",
    "    try:\n",
    "        # Wait up to 5 seconds for the \"Accept\" button\n",
    "        btn = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        print(\"      Banner Closed.\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"      No banner found (or already closed).\")\n",
    "\n",
    "def find_review_frame(driver):\n",
    "    \"\"\"\n",
    "    Checks ALL iframes to find the one containing the 'Load more' button.\n",
    "    Returns the index of the correct iframe.\n",
    "    \"\"\"\n",
    "    print(\"   -> Hunting for the 'Load more' button inside iframes...\")\n",
    "    \n",
    "    # 1. Check Main Page first\n",
    "    if is_button_present(driver):\n",
    "        print(\"      Found button on MAIN PAGE.\")\n",
    "        return -1 # -1 means \"Main Page\"\n",
    "    \n",
    "    # 2. Check Iframes\n",
    "    iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "    print(f\"      Found {len(iframes)} potential iframes. Checking them...\")\n",
    "    \n",
    "    for i, frame in enumerate(iframes):\n",
    "        try:\n",
    "            driver.switch_to.default_content()\n",
    "            driver.switch_to.frame(frame)\n",
    "            \n",
    "            # Scroll to trigger lazy loading\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            \n",
    "            if is_button_present(driver):\n",
    "                print(f\"      !!! FOUND BUTTON IN IFRAME #{i} !!!\")\n",
    "                return i\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    print(\"      Could not find button. It might be auto-loading or empty.\")\n",
    "    return None\n",
    "\n",
    "def is_button_present(driver):\n",
    "    \"\"\"Helper to check if the button exists and is visible.\"\"\"\n",
    "    # List of possible text Uniqlo uses\n",
    "    xpaths = [\n",
    "        \"//button[contains(text(), 'View more')]\",\n",
    "        \"//button[contains(text(), 'View More')]\",\n",
    "        \"//button[contains(text(), 'Load more')]\",\n",
    "        \"//div[contains(@class, 'load-more')]\",\n",
    "        \"//button[contains(@class, 'bv-content-btn-load-more')]\"\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, xpath)\n",
    "            if btn.is_displayed():\n",
    "                return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def click_load_more(driver):\n",
    "    \"\"\"Clicks the button if found.\"\"\"\n",
    "    xpaths = [\n",
    "        \"//button[contains(text(), 'View more')]\",\n",
    "        \"//button[contains(text(), 'View More')]\",\n",
    "        \"//button[contains(text(), 'Load more')]\",\n",
    "        \"//div[contains(@class, 'load-more')]\",\n",
    "        \"//button[contains(@class, 'bv-content-btn-load-more')]\"\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, xpath)\n",
    "            if btn.is_displayed():\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def scrape_data(driver_source):\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    reviews_data = []\n",
    "    \n",
    "    # Uniqlo reviews usually have \"out of 5 stars\" text\n",
    "    potential_blocks = soup.find_all(string=lambda text: text and \"out of 5 stars\" in text)\n",
    "    print(f\"   -> Extracting... Found {len(potential_blocks)} review blocks.\")\n",
    "\n",
    "    for rating_text in potential_blocks:\n",
    "        try:\n",
    "            container = rating_text.find_parent('div').find_parent('div')\n",
    "            if not container: continue\n",
    "\n",
    "            texts = list(container.stripped_strings)\n",
    "            \n",
    "            # Default Values\n",
    "            rating = rating_text.strip().split()[0]\n",
    "            published_at = \"N/A\"\n",
    "            author = \"Anonymous\"\n",
    "            feedback = \"\"\n",
    "            likes = 0\n",
    "            \n",
    "            for t in texts:\n",
    "                # Date (dd/mm/yyyy)\n",
    "                if len(t) == 10 and t[2] == '/' and t[5] == '/':\n",
    "                    published_at = t\n",
    "                # Author Info\n",
    "                if any(k in t for k in [\"Male\", \"Female\", \"Height\", \"Weight\", \"Shoe size\"]):\n",
    "                    author = t\n",
    "                # Likes\n",
    "                if \"Helpful\" in t and \"(\" in t:\n",
    "                     try: likes = int(t.split('(')[1].split(')')[0])\n",
    "                     except: pass\n",
    "            \n",
    "            # Feedback Body\n",
    "            possible_bodies = [t for t in texts if len(t) > 5 and t != author and \"out of 5\" not in t]\n",
    "            if possible_bodies:\n",
    "                feedback = max(possible_bodies, key=len)\n",
    "\n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": \"Asia\",\n",
    "                    \"Country\": \"Malaysia\",\n",
    "                    \"Source\": \"Uniqlo MY Official\",\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return reviews_data\n",
    "\n",
    "def process_product(driver, url):\n",
    "    print(f\"--- Accessing {url} ---\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    close_cookie_banner(driver)\n",
    "    \n",
    "    # 1. Find where the reviews are (Main Page or Iframe)\n",
    "    frame_index = find_review_frame(driver)\n",
    "    \n",
    "    # 2. Click Loop\n",
    "    click_count = 0\n",
    "    max_clicks = 100 # Adjust if there are thousands of reviews\n",
    "    \n",
    "    while click_count < max_clicks:\n",
    "        try:\n",
    "            # Ensure we are in the right frame\n",
    "            if frame_index != -1:\n",
    "                driver.switch_to.default_content()\n",
    "                frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "                if len(frames) > frame_index:\n",
    "                    driver.switch_to.frame(frames[frame_index])\n",
    "            \n",
    "            # Scroll to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            if click_load_more(driver):\n",
    "                print(f\"      Clicked 'Load more' ({click_count+1})\")\n",
    "                click_count += 1\n",
    "                # Wait for new reviews to load\n",
    "                time.sleep(3) \n",
    "            else:\n",
    "                print(\"      Button gone. All reviews loaded.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"      Loop Error: {e}\")\n",
    "            break\n",
    "            \n",
    "    # 3. Extract Data\n",
    "    print(\"   -> Parsing final data...\")\n",
    "    # Refresh frame context to get full source\n",
    "    if frame_index != -1:\n",
    "        driver.switch_to.default_content()\n",
    "        frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        if len(frames) > frame_index:\n",
    "            driver.switch_to.frame(frames[frame_index])\n",
    "            \n",
    "    html = driver.page_source\n",
    "    data = scrape_data(html)\n",
    "    \n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        # Remove duplicates\n",
    "        df.drop_duplicates(subset=['Feedback'], inplace=True)\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"SUCCESS! Saved {len(df)} reviews to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No reviews extracted. Check if selectors match.\")\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country\", \"Source\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    try:\n",
    "        process_product(driver, PRODUCT_URL)\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"\\nDONE.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa8eb8d2-0e16-4b43-94f7-fde5d6264304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Updated! Total Reviews: 977\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your MAIN dataset (Replace with your latest filename)\n",
    "df_main = pd.read_csv(\"UNIQLO_GLOBAL_REVIEWS_ALL_FINAL.csv\") \n",
    "\n",
    "# 2. Load the new Sweatshirt file\n",
    "df_new = pd.read_csv(\"uniqlo_my_sweatshirt_reviews.csv\")\n",
    "\n",
    "# 3. Combine\n",
    "df_final = pd.concat([df_main, df_new], ignore_index=True)\n",
    "\n",
    "# 4. Save\n",
    "df_final.to_csv(\"UNIQLO_PROJECT_DATASET_UPDATED.csv\", index=False)\n",
    "print(f\"Dataset Updated! Total Reviews: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d125a0d-0bed-46d5-84e2-87e16c1d7713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Accessing https://www.uniqlo.com/my/en/products/E465196-000/reviews ---\n",
      "   -> Checking for Cookie Banner...\n",
      "      Banner Closed.\n",
      "   -> Scanning for Review Iframe...\n",
      "      Found button on Main Page.\n",
      "      Clicked 'Load more' (1)\n",
      "      Clicked 'Load more' (2)\n",
      "      Clicked 'Load more' (3)\n",
      "      Clicked 'Load more' (4)\n",
      "      Clicked 'Load more' (5)\n",
      "      Clicked 'Load more' (6)\n",
      "      Clicked 'Load more' (7)\n",
      "      Clicked 'Load more' (8)\n",
      "      Clicked 'Load more' (9)\n",
      "      Clicked 'Load more' (10)\n",
      "      Clicked 'Load more' (11)\n",
      "      Clicked 'Load more' (12)\n",
      "      Clicked 'Load more' (13)\n",
      "      Clicked 'Load more' (14)\n",
      "      Clicked 'Load more' (15)\n",
      "      Clicked 'Load more' (16)\n",
      "      Clicked 'Load more' (17)\n",
      "      Clicked 'Load more' (18)\n",
      "      Clicked 'Load more' (19)\n",
      "      Clicked 'Load more' (20)\n",
      "      Clicked 'Load more' (21)\n",
      "      Clicked 'Load more' (22)\n",
      "      Clicked 'Load more' (23)\n",
      "      Clicked 'Load more' (24)\n",
      "      Clicked 'Load more' (25)\n",
      "      Clicked 'Load more' (26)\n",
      "      Button gone. All reviews loaded.\n",
      "   -> Extracting data...\n",
      "SUCCESS! Saved 140 reviews to uniqlo_my_E465196_reviews.csv\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_FILE = \"uniqlo_my_E465196_reviews.csv\"\n",
    "PRODUCT_URL = \"https://www.uniqlo.com/my/en/products/E465196-000/reviews\"\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    # Use 'normal' to ensure basic elements load before we start\n",
    "    options.page_load_strategy = 'normal' \n",
    "    driver = uc.Chrome(options=options)\n",
    "    # Set long timeouts to prevent \"Read timed out\"\n",
    "    driver.set_script_timeout(120)\n",
    "    driver.set_page_load_timeout(120)\n",
    "    return driver\n",
    "\n",
    "def close_cookie_banner(driver):\n",
    "    \"\"\"Closes the cookie banner if it exists.\"\"\"\n",
    "    print(\"   -> Checking for Cookie Banner...\")\n",
    "    try:\n",
    "        # Wait up to 8 seconds for the banner\n",
    "        btn = WebDriverWait(driver, 8).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        print(\"      Banner Closed.\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"      No banner found (or already closed).\")\n",
    "\n",
    "def find_review_frame(driver):\n",
    "    \"\"\"\n",
    "    Scans for the iframe containing the 'Load more' button.\n",
    "    Returns the index of the correct iframe.\n",
    "    \"\"\"\n",
    "    print(\"   -> Scanning for Review Iframe...\")\n",
    "    \n",
    "    # 1. Check Main Page\n",
    "    if is_button_visible(driver):\n",
    "        print(\"      Found button on Main Page.\")\n",
    "        return -1\n",
    "    \n",
    "    # 2. Check Iframes\n",
    "    iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "    print(f\"      Found {len(iframes)} iframes. Checking content...\")\n",
    "    \n",
    "    for i, frame in enumerate(iframes):\n",
    "        try:\n",
    "            driver.switch_to.default_content()\n",
    "            # Refetch to avoid stale elements\n",
    "            current_frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "            if i >= len(current_frames): break\n",
    "            \n",
    "            driver.switch_to.frame(current_frames[i])\n",
    "            \n",
    "            # Scroll to wake up the iframe\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if is_button_visible(driver):\n",
    "                print(f\"      !!! Found 'Load More' in Iframe #{i} !!!\")\n",
    "                return i\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    print(\"      Warning: Could not find 'Load More' button. (Maybe already loaded?)\")\n",
    "    return None\n",
    "\n",
    "def is_button_visible(driver):\n",
    "    \"\"\"Checks for the existence of the button.\"\"\"\n",
    "    xpaths = [\n",
    "        \"//button[contains(text(), 'Load more')]\",\n",
    "        \"//button[contains(text(), 'View more')]\",\n",
    "        \"//div[contains(@class, 'load-more')]\",\n",
    "        \"//button[contains(@class, 'bv-content-btn-load-more')]\"\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, xpath)\n",
    "            if btn.is_displayed(): return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def click_load_button(driver):\n",
    "    \"\"\"Clicks the button safely.\"\"\"\n",
    "    xpaths = [\n",
    "        \"//button[contains(text(), 'Load more')]\",\n",
    "        \"//button[contains(text(), 'View more')]\",\n",
    "        \"//div[contains(@class, 'load-more')]\",\n",
    "        \"//button[contains(@class, 'bv-content-btn-load-more')]\"\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, xpath)\n",
    "            if btn.is_displayed():\n",
    "                # Use JavaScript click to bypass overlays\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def scrape_data(driver_source):\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    reviews_data = []\n",
    "    \n",
    "    potential_blocks = soup.find_all(string=lambda text: text and \"out of 5 stars\" in text)\n",
    "    \n",
    "    for rating_text in potential_blocks:\n",
    "        try:\n",
    "            container = rating_text.find_parent('div').find_parent('div')\n",
    "            if not container: continue\n",
    "\n",
    "            texts = list(container.stripped_strings)\n",
    "            rating = rating_text.strip().split()[0]\n",
    "            published_at = \"N/A\"\n",
    "            author = \"Anonymous\"\n",
    "            feedback = \"\"\n",
    "            likes = 0\n",
    "            \n",
    "            for t in texts:\n",
    "                if len(t) == 10 and t[2] == '/' and t[5] == '/': published_at = t\n",
    "                if any(k in t for k in [\"Male\", \"Female\", \"Height\", \"Weight\"]): author = t\n",
    "                if \"Helpful\" in t and \"(\" in t:\n",
    "                     try: likes = int(t.split('(')[1].split(')')[0])\n",
    "                     except: pass\n",
    "            \n",
    "            possible_bodies = [t for t in texts if len(t) > 5 and t != author and \"out of 5\" not in t]\n",
    "            if possible_bodies: feedback = max(possible_bodies, key=len)\n",
    "\n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": \"Asia\",\n",
    "                    \"Country\": \"Malaysia\",\n",
    "                    \"Source\": \"Uniqlo MY Official\",\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "    return reviews_data\n",
    "\n",
    "def process_product(driver, url):\n",
    "    print(f\"--- Accessing {url} ---\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    close_cookie_banner(driver)\n",
    "    \n",
    "    # 1. Locate Frame\n",
    "    frame_index = find_review_frame(driver)\n",
    "    \n",
    "    # 2. Click Loop\n",
    "    click_count = 0\n",
    "    # Approx 750 reviews / 6 reviews per click = ~125 clicks\n",
    "    max_clicks = 150 \n",
    "    \n",
    "    while click_count < max_clicks:\n",
    "        try:\n",
    "            # Ensure we are in the correct frame\n",
    "            if frame_index != -1 and frame_index is not None:\n",
    "                driver.switch_to.default_content()\n",
    "                frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "                if len(frames) > frame_index:\n",
    "                    driver.switch_to.frame(frames[frame_index])\n",
    "            \n",
    "            # Scroll\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1) # Short pause before looking for button\n",
    "            \n",
    "            if click_load_button(driver):\n",
    "                print(f\"      Clicked 'Load more' ({click_count+1})\")\n",
    "                click_count += 1\n",
    "                # WAIT 3 SECONDS - Critical for stability\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"      Button gone. All reviews loaded.\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      Loop Error: {e}\")\n",
    "            break\n",
    "\n",
    "    # 3. Extract Final Data\n",
    "    print(\"   -> Extracting data...\")\n",
    "    if frame_index != -1 and frame_index is not None:\n",
    "        driver.switch_to.default_content()\n",
    "        frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        if len(frames) > frame_index:\n",
    "            driver.switch_to.frame(frames[frame_index])\n",
    "            \n",
    "    html = driver.page_source\n",
    "    data = scrape_data(html)\n",
    "    \n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.drop_duplicates(subset=['Feedback'], inplace=True)\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"SUCCESS! Saved {len(df)} reviews to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No reviews extracted.\")\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country\", \"Source\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    try:\n",
    "        process_product(driver, PRODUCT_URL)\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"\\nDONE.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f354d669-26fa-4733-a004-812c8ac06099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews: 1117\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your MAIN dataset (Check your file name!)\n",
    "df_main = pd.read_csv(\"UNIQLO_PROJECT_DATASET_UPDATED.csv\") \n",
    "\n",
    "# Load the new file\n",
    "df_new = pd.read_csv(\"uniqlo_my_E465196_reviews.csv\")\n",
    "\n",
    "# Combine\n",
    "df_final = pd.concat([df_main, df_new], ignore_index=True)\n",
    "\n",
    "# Save\n",
    "df_final.to_csv(\"UNIQLO_PROJECT_DATASET_ALL.csv\", index=False)\n",
    "print(f\"Total Reviews: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44d185c7-3eef-4430-bf36-75e8d7a6ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Accessing https://www.uniqlo.com/my/en/products/E447780-000/reviews ---\n",
      "   -> Checking for Cookie Banner...\n",
      "      Banner Closed.\n",
      "   -> Scanning for Review Iframe...\n",
      "      Found button on Main Page.\n",
      "      Clicked 'Load more' (1)\n",
      "      Clicked 'Load more' (2)\n",
      "      Clicked 'Load more' (3)\n",
      "      Clicked 'Load more' (4)\n",
      "      Clicked 'Load more' (5)\n",
      "      Clicked 'Load more' (6)\n",
      "      Clicked 'Load more' (7)\n",
      "      Clicked 'Load more' (8)\n",
      "      Clicked 'Load more' (9)\n",
      "      Clicked 'Load more' (10)\n",
      "      Clicked 'Load more' (11)\n",
      "      Clicked 'Load more' (12)\n",
      "      Clicked 'Load more' (13)\n",
      "      Clicked 'Load more' (14)\n",
      "      Clicked 'Load more' (15)\n",
      "      Clicked 'Load more' (16)\n",
      "      Clicked 'Load more' (17)\n",
      "      Clicked 'Load more' (18)\n",
      "      Clicked 'Load more' (19)\n",
      "      Clicked 'Load more' (20)\n",
      "      Clicked 'Load more' (21)\n",
      "      Clicked 'Load more' (22)\n",
      "      Clicked 'Load more' (23)\n",
      "      Clicked 'Load more' (24)\n",
      "      Clicked 'Load more' (25)\n",
      "      Clicked 'Load more' (26)\n",
      "      Clicked 'Load more' (27)\n",
      "      Clicked 'Load more' (28)\n",
      "      Clicked 'Load more' (29)\n",
      "      Button gone. All reviews loaded.\n",
      "   -> Extracting data...\n",
      "SUCCESS! Saved 155 reviews to uniqlo_my_E447780_reviews.csv\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_FILE = \"uniqlo_my_E447780_reviews.csv\"\n",
    "PRODUCT_URL = \"https://www.uniqlo.com/my/en/products/E447780-000/reviews\"\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    # Use 'normal' to ensure basic elements load before we start\n",
    "    options.page_load_strategy = 'normal' \n",
    "    driver = uc.Chrome(options=options)\n",
    "    # Set long timeouts to prevent \"Read timed out\"\n",
    "    driver.set_script_timeout(120)\n",
    "    driver.set_page_load_timeout(120)\n",
    "    return driver\n",
    "\n",
    "def close_cookie_banner(driver):\n",
    "    \"\"\"Closes the cookie banner if it exists.\"\"\"\n",
    "    print(\"   -> Checking for Cookie Banner...\")\n",
    "    try:\n",
    "        # Wait up to 8 seconds for the banner\n",
    "        btn = WebDriverWait(driver, 8).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        print(\"      Banner Closed.\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"      No banner found (or already closed).\")\n",
    "\n",
    "def find_review_frame(driver):\n",
    "    \"\"\"\n",
    "    Scans for the iframe containing the 'Load more' button.\n",
    "    Returns the index of the correct iframe.\n",
    "    \"\"\"\n",
    "    print(\"   -> Scanning for Review Iframe...\")\n",
    "    \n",
    "    # 1. Check Main Page\n",
    "    if is_button_visible(driver):\n",
    "        print(\"      Found button on Main Page.\")\n",
    "        return -1\n",
    "    \n",
    "    # 2. Check Iframes\n",
    "    iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "    print(f\"      Found {len(iframes)} iframes. Checking content...\")\n",
    "    \n",
    "    for i, frame in enumerate(iframes):\n",
    "        try:\n",
    "            driver.switch_to.default_content()\n",
    "            # Refetch to avoid stale elements\n",
    "            current_frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "            if i >= len(current_frames): break\n",
    "            \n",
    "            driver.switch_to.frame(current_frames[i])\n",
    "            \n",
    "            # Scroll to wake up the iframe\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if is_button_visible(driver):\n",
    "                print(f\"      !!! Found 'Load More' in Iframe #{i} !!!\")\n",
    "                return i\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    print(\"      Warning: Could not find 'Load More' button. (Maybe already loaded?)\")\n",
    "    return None\n",
    "\n",
    "def is_button_visible(driver):\n",
    "    \"\"\"Checks for the existence of the button.\"\"\"\n",
    "    xpaths = [\n",
    "        \"//button[contains(text(), 'Load more')]\",\n",
    "        \"//button[contains(text(), 'View more')]\",\n",
    "        \"//div[contains(@class, 'load-more')]\",\n",
    "        \"//button[contains(@class, 'bv-content-btn-load-more')]\"\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, xpath)\n",
    "            if btn.is_displayed(): return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def click_load_button(driver):\n",
    "    \"\"\"Clicks the button safely.\"\"\"\n",
    "    xpaths = [\n",
    "        \"//button[contains(text(), 'Load more')]\",\n",
    "        \"//button[contains(text(), 'View more')]\",\n",
    "        \"//div[contains(@class, 'load-more')]\",\n",
    "        \"//button[contains(@class, 'bv-content-btn-load-more')]\"\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, xpath)\n",
    "            if btn.is_displayed():\n",
    "                # Use JavaScript click to bypass overlays\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def scrape_data(driver_source):\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    reviews_data = []\n",
    "    \n",
    "    potential_blocks = soup.find_all(string=lambda text: text and \"out of 5 stars\" in text)\n",
    "    \n",
    "    for rating_text in potential_blocks:\n",
    "        try:\n",
    "            container = rating_text.find_parent('div').find_parent('div')\n",
    "            if not container: continue\n",
    "\n",
    "            texts = list(container.stripped_strings)\n",
    "            rating = rating_text.strip().split()[0]\n",
    "            published_at = \"N/A\"\n",
    "            author = \"Anonymous\"\n",
    "            feedback = \"\"\n",
    "            likes = 0\n",
    "            \n",
    "            for t in texts:\n",
    "                if len(t) == 10 and t[2] == '/' and t[5] == '/': published_at = t\n",
    "                if any(k in t for k in [\"Male\", \"Female\", \"Height\", \"Weight\"]): author = t\n",
    "                if \"Helpful\" in t and \"(\" in t:\n",
    "                     try: likes = int(t.split('(')[1].split(')')[0])\n",
    "                     except: pass\n",
    "            \n",
    "            possible_bodies = [t for t in texts if len(t) > 5 and t != author and \"out of 5\" not in t]\n",
    "            if possible_bodies: feedback = max(possible_bodies, key=len)\n",
    "\n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": \"Asia\",\n",
    "                    \"Country\": \"Malaysia\",\n",
    "                    \"Source\": \"Uniqlo MY Official\",\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "    return reviews_data\n",
    "\n",
    "def process_product(driver, url):\n",
    "    print(f\"--- Accessing {url} ---\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    close_cookie_banner(driver)\n",
    "    \n",
    "    # 1. Locate Frame\n",
    "    frame_index = find_review_frame(driver)\n",
    "    \n",
    "    # 2. Click Loop\n",
    "    click_count = 0\n",
    "    # Approx 750 reviews / 6 reviews per click = ~125 clicks\n",
    "    max_clicks = 200 \n",
    "    \n",
    "    while click_count < max_clicks:\n",
    "        try:\n",
    "            # Ensure we are in the correct frame\n",
    "            if frame_index != -1 and frame_index is not None:\n",
    "                driver.switch_to.default_content()\n",
    "                frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "                if len(frames) > frame_index:\n",
    "                    driver.switch_to.frame(frames[frame_index])\n",
    "            \n",
    "            # Scroll\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1) # Short pause before looking for button\n",
    "            \n",
    "            if click_load_button(driver):\n",
    "                print(f\"      Clicked 'Load more' ({click_count+1})\")\n",
    "                click_count += 1\n",
    "                # WAIT 3 SECONDS - Critical for stability\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"      Button gone. All reviews loaded.\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      Loop Error: {e}\")\n",
    "            break\n",
    "\n",
    "    # 3. Extract Final Data\n",
    "    print(\"   -> Extracting data...\")\n",
    "    if frame_index != -1 and frame_index is not None:\n",
    "        driver.switch_to.default_content()\n",
    "        frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        if len(frames) > frame_index:\n",
    "            driver.switch_to.frame(frames[frame_index])\n",
    "            \n",
    "    html = driver.page_source\n",
    "    data = scrape_data(html)\n",
    "    \n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.drop_duplicates(subset=['Feedback'], inplace=True)\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"SUCCESS! Saved {len(df)} reviews to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No reviews extracted.\")\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country\", \"Source\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    try:\n",
    "        process_product(driver, PRODUCT_URL)\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"\\nDONE.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71b5fcaf-552a-438e-b32f-fcb35ab4c54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews: 1272\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your MAIN dataset (Check your file name!)\n",
    "df_main = pd.read_csv(\"UNIQLO_PROJECT_DATASET_ALL.csv\") \n",
    "\n",
    "# Load the new file\n",
    "df_new = pd.read_csv(\"uniqlo_my_E447780_reviews.csv\")\n",
    "\n",
    "# Combine\n",
    "df_final = pd.concat([df_main, df_new], ignore_index=True)\n",
    "\n",
    "# Save\n",
    "df_final.to_csv(\"UNIQLO_PROJECT_DATASET_ALL2.csv\", index=False)\n",
    "print(f\"Total Reviews: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48b5c5f8-15b1-4444-92bd-9e9f6af9b334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Accessing https://www.uniqlo.com/my/en/products/E466026-000/reviews ---\n",
      "   -> Checking for Cookie Banner...\n",
      "      Banner Closed.\n",
      "   -> Scanning for Review Iframe...\n",
      "      Found button on Main Page.\n",
      "      Clicked 'Load more' (1)\n",
      "      Clicked 'Load more' (2)\n",
      "      Clicked 'Load more' (3)\n",
      "      Clicked 'Load more' (4)\n",
      "      Clicked 'Load more' (5)\n",
      "      Clicked 'Load more' (6)\n",
      "      Clicked 'Load more' (7)\n",
      "      Clicked 'Load more' (8)\n",
      "      Clicked 'Load more' (9)\n",
      "      Clicked 'Load more' (10)\n",
      "      Clicked 'Load more' (11)\n",
      "      Clicked 'Load more' (12)\n",
      "      Clicked 'Load more' (13)\n",
      "      Clicked 'Load more' (14)\n",
      "      Clicked 'Load more' (15)\n",
      "      Clicked 'Load more' (16)\n",
      "      Clicked 'Load more' (17)\n",
      "      Clicked 'Load more' (18)\n",
      "      Clicked 'Load more' (19)\n",
      "      Clicked 'Load more' (20)\n",
      "      Clicked 'Load more' (21)\n",
      "      Clicked 'Load more' (22)\n",
      "      Clicked 'Load more' (23)\n",
      "      Button gone. All reviews loaded.\n",
      "   -> Extracting data...\n",
      "SUCCESS! Saved 126 reviews to uniqlo_my_E466026_reviews.csv\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_FILE = \"uniqlo_my_E466026_reviews.csv\"\n",
    "PRODUCT_URL = \"https://www.uniqlo.com/my/en/products/E466026-000/reviews\"\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    # Use 'normal' to ensure basic elements load before we start\n",
    "    options.page_load_strategy = 'normal' \n",
    "    driver = uc.Chrome(options=options)\n",
    "    # Set long timeouts to prevent \"Read timed out\"\n",
    "    driver.set_script_timeout(120)\n",
    "    driver.set_page_load_timeout(120)\n",
    "    return driver\n",
    "\n",
    "def close_cookie_banner(driver):\n",
    "    \"\"\"Closes the cookie banner if it exists.\"\"\"\n",
    "    print(\"   -> Checking for Cookie Banner...\")\n",
    "    try:\n",
    "        # Wait up to 8 seconds for the banner\n",
    "        btn = WebDriverWait(driver, 8).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        print(\"      Banner Closed.\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"      No banner found (or already closed).\")\n",
    "\n",
    "def find_review_frame(driver):\n",
    "    \"\"\"\n",
    "    Scans for the iframe containing the 'Load more' button.\n",
    "    Returns the index of the correct iframe.\n",
    "    \"\"\"\n",
    "    print(\"   -> Scanning for Review Iframe...\")\n",
    "    \n",
    "    # 1. Check Main Page\n",
    "    if is_button_visible(driver):\n",
    "        print(\"      Found button on Main Page.\")\n",
    "        return -1\n",
    "    \n",
    "    # 2. Check Iframes\n",
    "    iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "    print(f\"      Found {len(iframes)} iframes. Checking content...\")\n",
    "    \n",
    "    for i, frame in enumerate(iframes):\n",
    "        try:\n",
    "            driver.switch_to.default_content()\n",
    "            # Refetch to avoid stale elements\n",
    "            current_frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "            if i >= len(current_frames): break\n",
    "            \n",
    "            driver.switch_to.frame(current_frames[i])\n",
    "            \n",
    "            # Scroll to wake up the iframe\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if is_button_visible(driver):\n",
    "                print(f\"      !!! Found 'Load More' in Iframe #{i} !!!\")\n",
    "                return i\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    print(\"      Warning: Could not find 'Load More' button. (Maybe already loaded?)\")\n",
    "    return None\n",
    "\n",
    "def is_button_visible(driver):\n",
    "    \"\"\"Checks for the existence of the button.\"\"\"\n",
    "    xpaths = [\n",
    "        \"//button[contains(text(), 'Load more')]\",\n",
    "        \"//button[contains(text(), 'View more')]\",\n",
    "        \"//div[contains(@class, 'load-more')]\",\n",
    "        \"//button[contains(@class, 'bv-content-btn-load-more')]\"\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, xpath)\n",
    "            if btn.is_displayed(): return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def click_load_button(driver):\n",
    "    \"\"\"Clicks the button safely.\"\"\"\n",
    "    xpaths = [\n",
    "        \"//button[contains(text(), 'Load more')]\",\n",
    "        \"//button[contains(text(), 'View more')]\",\n",
    "        \"//div[contains(@class, 'load-more')]\",\n",
    "        \"//button[contains(@class, 'bv-content-btn-load-more')]\"\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, xpath)\n",
    "            if btn.is_displayed():\n",
    "                # Use JavaScript click to bypass overlays\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def scrape_data(driver_source):\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    reviews_data = []\n",
    "    \n",
    "    potential_blocks = soup.find_all(string=lambda text: text and \"out of 5 stars\" in text)\n",
    "    \n",
    "    for rating_text in potential_blocks:\n",
    "        try:\n",
    "            container = rating_text.find_parent('div').find_parent('div')\n",
    "            if not container: continue\n",
    "\n",
    "            texts = list(container.stripped_strings)\n",
    "            rating = rating_text.strip().split()[0]\n",
    "            published_at = \"N/A\"\n",
    "            author = \"Anonymous\"\n",
    "            feedback = \"\"\n",
    "            likes = 0\n",
    "            \n",
    "            for t in texts:\n",
    "                if len(t) == 10 and t[2] == '/' and t[5] == '/': published_at = t\n",
    "                if any(k in t for k in [\"Male\", \"Female\", \"Height\", \"Weight\"]): author = t\n",
    "                if \"Helpful\" in t and \"(\" in t:\n",
    "                     try: likes = int(t.split('(')[1].split(')')[0])\n",
    "                     except: pass\n",
    "            \n",
    "            possible_bodies = [t for t in texts if len(t) > 5 and t != author and \"out of 5\" not in t]\n",
    "            if possible_bodies: feedback = max(possible_bodies, key=len)\n",
    "\n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": \"Asia\",\n",
    "                    \"Country\": \"Malaysia\",\n",
    "                    \"Source\": \"Uniqlo MY Official\",\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "    return reviews_data\n",
    "\n",
    "def process_product(driver, url):\n",
    "    print(f\"--- Accessing {url} ---\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    close_cookie_banner(driver)\n",
    "    \n",
    "    # 1. Locate Frame\n",
    "    frame_index = find_review_frame(driver)\n",
    "    \n",
    "    # 2. Click Loop\n",
    "    click_count = 0\n",
    "    # Approx 750 reviews / 6 reviews per click = ~125 clicks\n",
    "    max_clicks = 500 \n",
    "    \n",
    "    while click_count < max_clicks:\n",
    "        try:\n",
    "            # Ensure we are in the correct frame\n",
    "            if frame_index != -1 and frame_index is not None:\n",
    "                driver.switch_to.default_content()\n",
    "                frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "                if len(frames) > frame_index:\n",
    "                    driver.switch_to.frame(frames[frame_index])\n",
    "            \n",
    "            # Scroll\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1) # Short pause before looking for button\n",
    "            \n",
    "            if click_load_button(driver):\n",
    "                print(f\"      Clicked 'Load more' ({click_count+1})\")\n",
    "                click_count += 1\n",
    "                # WAIT 3 SECONDS - Critical for stability\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"      Button gone. All reviews loaded.\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      Loop Error: {e}\")\n",
    "            break\n",
    "\n",
    "    # 3. Extract Final Data\n",
    "    print(\"   -> Extracting data...\")\n",
    "    if frame_index != -1 and frame_index is not None:\n",
    "        driver.switch_to.default_content()\n",
    "        frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        if len(frames) > frame_index:\n",
    "            driver.switch_to.frame(frames[frame_index])\n",
    "            \n",
    "    html = driver.page_source\n",
    "    data = scrape_data(html)\n",
    "    \n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.drop_duplicates(subset=['Feedback'], inplace=True)\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"SUCCESS! Saved {len(df)} reviews to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No reviews extracted.\")\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country\", \"Source\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    try:\n",
    "        process_product(driver, PRODUCT_URL)\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"\\nDONE.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d875894b-8367-475a-8018-18e649a2cd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews: 1398\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your MAIN dataset (Check your file name!)\n",
    "df_main = pd.read_csv(\"UNIQLO_PROJECT_DATASET_ALL2.csv\") \n",
    "\n",
    "# Load the new file\n",
    "df_new = pd.read_csv(\"uniqlo_my_E466026_reviews.csv\")\n",
    "\n",
    "# Combine\n",
    "df_final = pd.concat([df_main, df_new], ignore_index=True)\n",
    "\n",
    "# Save\n",
    "df_final.to_csv(\"UNIQLO_PROJECT_DATASET_ALL3.csv\", index=False)\n",
    "print(f\"Total Reviews: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5082e2d-741a-4143-9655-4aceb18eadb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Accessing https://www.uniqlo.com/my/en/products/E455492-000/reviews ---\n",
      "   -> Checking for Cookie Banner...\n",
      "      Banner Closed.\n",
      "   -> Scanning for Review Iframe...\n",
      "      Found button on Main Page.\n",
      "      Clicked 'Load more' (1)\n",
      "      Clicked 'Load more' (2)\n",
      "      Clicked 'Load more' (3)\n",
      "      Clicked 'Load more' (4)\n",
      "      Clicked 'Load more' (5)\n",
      "      Clicked 'Load more' (6)\n",
      "      Clicked 'Load more' (7)\n",
      "      Clicked 'Load more' (8)\n",
      "      Clicked 'Load more' (9)\n",
      "      Clicked 'Load more' (10)\n",
      "      Clicked 'Load more' (11)\n",
      "      Clicked 'Load more' (12)\n",
      "      Clicked 'Load more' (13)\n",
      "      Clicked 'Load more' (14)\n",
      "      Clicked 'Load more' (15)\n",
      "      Clicked 'Load more' (16)\n",
      "      Clicked 'Load more' (17)\n",
      "      Clicked 'Load more' (18)\n",
      "      Clicked 'Load more' (19)\n",
      "      Clicked 'Load more' (20)\n",
      "      Clicked 'Load more' (21)\n",
      "      Clicked 'Load more' (22)\n",
      "      Clicked 'Load more' (23)\n",
      "      Clicked 'Load more' (24)\n",
      "      Clicked 'Load more' (25)\n",
      "      Button gone. All reviews loaded.\n",
      "   -> Extracting data...\n",
      "SUCCESS! Saved 134 reviews to uniqlo_my_E455492_reviews.csv\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_FILE = \"uniqlo_my_E455492_reviews.csv\"\n",
    "PRODUCT_URL = \"https://www.uniqlo.com/my/en/products/E455492-000/reviews\"\n",
    "\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    # Use 'normal' to ensure basic elements load before we start\n",
    "    options.page_load_strategy = 'normal' \n",
    "    driver = uc.Chrome(options=options)\n",
    "    # Set long timeouts to prevent \"Read timed out\"\n",
    "    driver.set_script_timeout(120)\n",
    "    driver.set_page_load_timeout(120)\n",
    "    return driver\n",
    "\n",
    "def close_cookie_banner(driver):\n",
    "    \"\"\"Closes the cookie banner if it exists.\"\"\"\n",
    "    print(\"   -> Checking for Cookie Banner...\")\n",
    "    try:\n",
    "        # Wait up to 8 seconds for the banner\n",
    "        btn = WebDriverWait(driver, 8).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        print(\"      Banner Closed.\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"      No banner found (or already closed).\")\n",
    "\n",
    "def find_review_frame(driver):\n",
    "    \"\"\"\n",
    "    Scans for the iframe containing the 'Load more' button.\n",
    "    Returns the index of the correct iframe.\n",
    "    \"\"\"\n",
    "    print(\"   -> Scanning for Review Iframe...\")\n",
    "    \n",
    "    # 1. Check Main Page\n",
    "    if is_button_visible(driver):\n",
    "        print(\"      Found button on Main Page.\")\n",
    "        return -1\n",
    "    \n",
    "    # 2. Check Iframes\n",
    "    iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "    print(f\"      Found {len(iframes)} iframes. Checking content...\")\n",
    "    \n",
    "    for i, frame in enumerate(iframes):\n",
    "        try:\n",
    "            driver.switch_to.default_content()\n",
    "            # Refetch to avoid stale elements\n",
    "            current_frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "            if i >= len(current_frames): break\n",
    "            \n",
    "            driver.switch_to.frame(current_frames[i])\n",
    "            \n",
    "            # Scroll to wake up the iframe\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if is_button_visible(driver):\n",
    "                print(f\"      !!! Found 'Load More' in Iframe #{i} !!!\")\n",
    "                return i\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    print(\"      Warning: Could not find 'Load More' button. (Maybe already loaded?)\")\n",
    "    return None\n",
    "\n",
    "def is_button_visible(driver):\n",
    "    \"\"\"Checks for the existence of the button.\"\"\"\n",
    "    xpaths = [\n",
    "        \"//button[contains(text(), 'Load more')]\",\n",
    "        \"//button[contains(text(), 'View more')]\",\n",
    "        \"//div[contains(@class, 'load-more')]\",\n",
    "        \"//button[contains(@class, 'bv-content-btn-load-more')]\"\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, xpath)\n",
    "            if btn.is_displayed(): return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def click_load_button(driver):\n",
    "    \"\"\"Clicks the button safely.\"\"\"\n",
    "    xpaths = [\n",
    "        \"//button[contains(text(), 'Load more')]\",\n",
    "        \"//button[contains(text(), 'View more')]\",\n",
    "        \"//div[contains(@class, 'load-more')]\",\n",
    "        \"//button[contains(@class, 'bv-content-btn-load-more')]\"\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, xpath)\n",
    "            if btn.is_displayed():\n",
    "                # Use JavaScript click to bypass overlays\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def scrape_data(driver_source):\n",
    "    soup = BeautifulSoup(driver_source, 'html.parser')\n",
    "    reviews_data = []\n",
    "    \n",
    "    potential_blocks = soup.find_all(string=lambda text: text and \"out of 5 stars\" in text)\n",
    "    \n",
    "    for rating_text in potential_blocks:\n",
    "        try:\n",
    "            container = rating_text.find_parent('div').find_parent('div')\n",
    "            if not container: continue\n",
    "\n",
    "            texts = list(container.stripped_strings)\n",
    "            rating = rating_text.strip().split()[0]\n",
    "            published_at = \"N/A\"\n",
    "            author = \"Anonymous\"\n",
    "            feedback = \"\"\n",
    "            likes = 0\n",
    "            \n",
    "            for t in texts:\n",
    "                if len(t) == 10 and t[2] == '/' and t[5] == '/': published_at = t\n",
    "                if any(k in t for k in [\"Male\", \"Female\", \"Height\", \"Weight\"]): author = t\n",
    "                if \"Helpful\" in t and \"(\" in t:\n",
    "                     try: likes = int(t.split('(')[1].split(')')[0])\n",
    "                     except: pass\n",
    "            \n",
    "            possible_bodies = [t for t in texts if len(t) > 5 and t != author and \"out of 5\" not in t]\n",
    "            if possible_bodies: feedback = max(possible_bodies, key=len)\n",
    "\n",
    "            if feedback:\n",
    "                reviews_data.append({\n",
    "                    \"Continent\": \"Asia\",\n",
    "                    \"Country\": \"Malaysia\",\n",
    "                    \"Source\": \"Uniqlo MY Official\",\n",
    "                    \"Author\": author,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Published At\": published_at,\n",
    "                    \"Like Count\": likes,\n",
    "                    \"Feedback\": feedback\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "    return reviews_data\n",
    "\n",
    "def process_product(driver, url):\n",
    "    print(f\"--- Accessing {url} ---\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    close_cookie_banner(driver)\n",
    "    \n",
    "    # 1. Locate Frame\n",
    "    frame_index = find_review_frame(driver)\n",
    "    \n",
    "    # 2. Click Loop\n",
    "    click_count = 0\n",
    "    # Approx 750 reviews / 6 reviews per click = ~125 clicks\n",
    "    max_clicks = 500 \n",
    "    \n",
    "    while click_count < max_clicks:\n",
    "        try:\n",
    "            # Ensure we are in the correct frame\n",
    "            if frame_index != -1 and frame_index is not None:\n",
    "                driver.switch_to.default_content()\n",
    "                frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "                if len(frames) > frame_index:\n",
    "                    driver.switch_to.frame(frames[frame_index])\n",
    "            \n",
    "            # Scroll\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1) # Short pause before looking for button\n",
    "            \n",
    "            if click_load_button(driver):\n",
    "                print(f\"      Clicked 'Load more' ({click_count+1})\")\n",
    "                click_count += 1\n",
    "                # WAIT 3 SECONDS - Critical for stability\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"      Button gone. All reviews loaded.\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      Loop Error: {e}\")\n",
    "            break\n",
    "\n",
    "    # 3. Extract Final Data\n",
    "    print(\"   -> Extracting data...\")\n",
    "    if frame_index != -1 and frame_index is not None:\n",
    "        driver.switch_to.default_content()\n",
    "        frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        if len(frames) > frame_index:\n",
    "            driver.switch_to.frame(frames[frame_index])\n",
    "            \n",
    "    html = driver.page_source\n",
    "    data = scrape_data(html)\n",
    "    \n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.drop_duplicates(subset=['Feedback'], inplace=True)\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"SUCCESS! Saved {len(df)} reviews to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No reviews extracted.\")\n",
    "\n",
    "def main():\n",
    "    driver = get_driver()\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        pd.DataFrame(columns=[\"Continent\", \"Country\", \"Source\", \"Author\", \"Rating\", \"Published At\", \"Like Count\", \"Feedback\"]).to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    try:\n",
    "        process_product(driver, PRODUCT_URL)\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(f\"\\nDONE.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f31e3f5-292a-4a56-91ce-7f86c86b6a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews: 1532\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your MAIN dataset (Check your file name!)\n",
    "df_main = pd.read_csv(\"UNIQLO_PROJECT_DATASET_ALL3.csv\") \n",
    "\n",
    "# Load the new file\n",
    "df_new = pd.read_csv(\"uniqlo_my_E455492_reviews.csv\")\n",
    "\n",
    "# Combine\n",
    "df_final = pd.concat([df_main, df_new], ignore_index=True)\n",
    "\n",
    "# Save\n",
    "df_final.to_csv(\"UNIQLO_PROJECT_DATASET_ALL4.csv\", index=False)\n",
    "print(f\"Total Reviews: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425980c0-acd9-4610-abe0-a26b96b213b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
